ntroduction
You are a Data Scientist struggling with data, code, and models in your projects.

You are an ML Engineer who has trouble replicating pipelines and monitoring models in production.

You are Head of a Data Science team, who has trouble shipping quickly models to production.

You are a CTO who has difficulty to move Python code into large-scale production and manage the DevOps workload on an AI project.

ðŸš€ Then, you are a potential user of Craft AI's MLOps platform, which aims to accelerate the deployment and improve the management of your Machine Learning models in production.

Find all the information you need
 Get started

Learn the basics and launch your first script on the platform.

 Getting started

 User Workflow

List of user workflows for creating deployments, using data, customising your code, etc.

 Introduction user worklflow

 Step & Pipeline

Understand the ML process and manage your steps & pipelines.

 Discover the concept

 Deployment

Deploy models seamlessly with our efficient deployment tools.

 Discover the concept

 Environment

Easily configure and optimise your ML environment for peak performance.

 Discover the concept

 Platform

Access our powerful AI platform and unleash its capabilities.

 Access to the platform

What is Craft AI?
Craft AI is an MLOps platform for data science teams.

To use Craft AI, the basic workflow with the platform is:

Choose a configured environment on the cloud provider of your choice
Create Machine Learning pipelines with your Python code
Deploy and execute the pipelines on environments running on Kubernetes
Monitor the performance of the models in production and the health of the infrastructure
The platform aims to be an end-to-end MLOps tool that brings together all the MLOps functionalities required for the successful implementation of an AI project. It is therefore composed of the following main features:

Environments
Machine Learning Pipelines
Deployments
platform_info

Mission
Our mission is to democratize the use of trustworthy Artificial Intelligence on a day-to-day basis. How do we do this? By empowering Data Science teams to master their AI project from start to finish. Our vision of AI is that every project of Data Science should be in production, responsible and profitable!

To achieve this, we have developed a MLOps platform that allows anyone to put Python code into production, on a large scale, in a few clicks. We allow Data Scientist to deploy their models, choose their environments (development and production) and create pipelines to optimize their ML workflows.

How does it work? We will contain your code in step to allow you to create ready to production ML and DL pipelines in an environment adapted to each projectâ€™s needs.

History
Craft AI is a French company founded in 2015. We originally developed AI solutions for the energy, industry, healthcare, education and retail sectors. Our unique ability to deploy thousands of ML models at large scale with a focus on being always explainable, energy frugal and fair, drove us to develop a MLOps platform as a front end to our expertise. Today, with our MLOps platform, we can share our expertise in large-scale model deployment, at large scale.

What is MLOps?
Machine Learning Operations (MLOps), aims to provide an end-to-end development process to design, build and manage reproducible, testable, and evolvable ML-powered software.

Being an emerging field, MLOps is rapidly gaining momentum amongst Data Scientists, ML Engineers and AI enthusiasts. Following this trend, MLOps differentiates the ML models management from traditional software engineering like DevOps and suggests the following MLOps capabilities:

MLOps aims to unify the release/production cycle for ML and software application release.
MLOps enables automated testing of machine learning artifacts (e.g. data validation, ML model testing, and ML model integration testing)ML
MLOps enables the application of agile principles to machine learning projects.
MLOps enables supporting machine learning models and datasets to build these models as first-class citizens within CI/CD systems.
MLOps reduces technical debt across machine learning models.
MLOps must be a language-, framework-, platform-, and infrastructure-agnostic practice.
Benefits of using Craft AI
The main benefits of the platform for the users are:

No longer taking 6 months to deploy ML models in production but only a few clicks!
Allowing a Data Science team to be autonomous on AI in production without DevOps skills.
Enabling large scale production of Python code without refactoring to Java or C.
Automating the execution of the pipelines to save time for Data Science teams.
Ensuring an efficient use of computing resources and reduce the cloud bill.
Improving model performance over time by automatically triggering re-training pipelines when performance drops.


Administration
The MLOps - Craft AI Platform is composed of a graphical interface allowing you to visualize, create, manage and monitor the objects necessary for the realization of your AI projects.

It is composed of:

Homepage: Visualize and create projects.
Parameters: Manage the companyâ€™s account and users.
Project settings: Manage the configuration of a project.
Environments: See the environment(s) and their information within a project.
Executions: See the execution(s) and their information within an environment or a deployment.
Note: We strongly recommend Google Chrome browser to use the graphical interface of the platform.

Summary:

Users
Projects
Token SDK
Users
Summary:

Manage user
Login
Get user with ID
Manage user
The management of user is available by email for the moment. In a next version, it will be possible to add, edit and delete a user directly on the platform UI.

Add a user
Send a message to Craft AI with your request and the following information:

First and last name of the user
Email of the user
Note

Adding users will arrive later on the platform.

Access rights
Each user has access to one or more defined projects.

Each user who has access to a project has access to all the information and actions in it.

Note

Advanced access rights will arrive later on the platform.

Delete a user
Send a message to Craft AI with your request and the following information:

First and last name of the user
Email of the user
Note

The deletion of users will arrive later on the platform.

Login
Here is the URL to access the platform: https://mlops-platform.craft.ai

First connection
Prerequisites: The user must be added to the platform (Add a user).

The user connects to the platform: https://mlops-platform.craft.ai The connection will not work, but Craft AI will receive the request and can add the user.
The user receives an email/slack from Craft AI to inform him that he can log in.
The user logs in with the same link and has access to the platform.
Forgot your password
In the Login popup, click on Donâ€™t remember your password?
Enter your email, you will receive an email to modify your password.
Get user with ID
Function definition
While using the SDK you may encounter outputs parameters containing a user when using specific functions. Each user is identified with a unique ID. That is the case for example in the sdk.get_pipeline_execution() function output with the parameter created_by. To match the user ID with the corresponding information (name and email) you can use the get_user() function.

CraftAiSdk.get_user(user_id)
Parameters
user_id (str) â€“ The ID of the user.
Returns
The user information in dict type, with the following keys:

id (str): ID of the user.
name (str): Name of the user.
email (str): Email of the user.
Projects
A project is a complete use case. From data import, through experimentation, testing and production, to performance monitoring over time.

Users can create as many projects as they want. Each project is completely isolated from the others.

To work on a project, it must include at least one environment. The user can create several environments in a project.

Summary:

Create a project
Manage a project
Create a project
From the homepage, you can create a new project by clicking on the â€œNew projectâ€ button.

A project creation page opens in which you have to fill in the fields :

[Mandatory]Project name : Enter the name of your project (in lowercase and with â€œ-â€), you will not be able to modify it later.
The following properties are default settings for the pipelines that will be created in this project, so you won't need to configure them each time you create a pipeline. It will always be possible to override these settings separately for each pipeline.

[Mandatory]Python version : Select the version of Python that will be applied to this project. It will be possible to choose a different version when creating each pipeline.

Repository URL : The SSH URL of a Git repository that contains code for your pipelines, starting with â€œgit@â€. When creating pipelines, it will also be possible to send code directly to the platform or to choose a different Git repository URL.

Deploy key : GitHub / GitLab private key to access a Git repository that contains code for your pipelines.

Default branch : The Git branch where to get pipeline code by default for this project. If this field is empty but a Git repository is used to get pipeline code, the platform will use the default Git branch. It will be possible to choose a different default branch within an environment.

[Mandatory]Folders : File(s) or folder(s) that will be included in pipelines, among the ones fetched from Git by the platform or sent by the user directly. By default, it will contain the value /, which means all the files in the folder or repository are included. It will be possible to choose different files or folders within an environment.

Requirements.txt : The path to the requirements.txt file with the Python libraries to install automatically on this project, among the pipeline files included in the folders defined above. It will be possible to choose a different file within an environment.

System dependencies : List of APT packages that you want to install automatically on the Linux system where pipelines will run in this project. It will be possible to add different packages within an environment.

Note

For more information on how to link pipelines with a Git repository, the full procedure is available here.

Click on â€œCreate projectâ€, you will see the project card displayed, and you can enter in it to create a first environment.

There are no access rights at first, all users of the platform have access to all projects.

administration_3

Note: You will be able to modify these elements (except the project name) in the Settings section of your project.

Next step: Create an environment in this project.

Manage a project
Edit a project
Within your project, click on Settings to view and edit your project information.

administration_4

On this page, you will find the information you set up when you created the project.

You can change them, except the name of the project.

Warning

Donâ€™t forget to save and validate the confirmation slider to make the changes effective.

The changes will apply to steps created after this modification. These changes may affect your steps and can make them non-functional.

administration_5

Users in a project
Initially, all users of the platform have access to all projects without special access rights.

Note

The access rights per user within a project will arrive later on the platform.

Delete a project
Initially, you cannot delete a project.

Submit an email request to delete a project from the platform.

Note

The deletion of a project will arrive later on the platform.

Access the SDK
The Craft AI MLOps Platform is composed of a Python SDK allowing you to use, from your IDE, the functions developed by Craft AI to create your steps, pipelines and deployments.

Summary:

Get token access
Connect to the SDK
Get token access
In the header, click on your name and go to the â€œParametersâ€ page.

administration_6

At the bottom of the â€œAccountâ€ page, you will find your SDK token, which will allow you to identify yourself to use the SDK.

You can regenerate it, the old token will be obsolete, and you will have to identify yourself again to the SDK to access it.

Warning

The SDK token is strictly personal. You must keep it to yourself.

administration_7

Connect to the SDK
You must install the SDK from a Python terminal, with the command :

pip install craft-ai-sdk
Run the following commands in a Python terminal to initialize the SDK.

Enter the CRAFT_AI_ENVIRONMENT_URL, you can find on Get environment URL.
It should look like https://brisk-hawk-charcoal-zeta-42.mlops-platform.craft.ai.

If you donâ€™t have any environments in your project yet, you should Create an environment.

Enter your personal CRAFT_AI_SDK_TOKEN, you can find it in "Parameters" on the platform web UI.
export CRAFT_AI_ENVIRONMENT_URL="*your-env-URL*"
export CRAFT_AI_SDK_TOKEN="*your-token-acces*"
Execute the following Python code to set up SDK Python object.
The SDK will automatically take into account your environment variables for the installation of the connection to the platform.

import os
from craft_ai_sdk import CraftAiSdk

sdk = CraftAiSdk()
You can also specify it directly in the constructor, although this method is not recommended.

import os
from craft_ai_sdk import CraftAiSdk

sdk = CraftAiSdk(
    environment_url="*your-env-URL*",
    sdk_token="*your-token-acces*"
)



Get Started
The goal of this use case is to accompany you in understanding the mechanics of the platform with a simple and basic machine learning application.

The final goal of this get started will be to be able to generate predictions on the Craft AI platform. The model used to generate predictions will be the iris one, but you can use any Python code.

To achieve this goal, we will go through several parts:

Part 0 : Setup
This page is about the setup of the platform in your Python code and in the Craft AI platform UI.

There are 2 ways to access the platform:

With the Python SDK, in line of code
With the web interface, in a browser
Prerequisites

Python 3.8 or higher is required to be installed on your computer.
We strongly recommend the use of Google Chrome browser to use the UI of the platform.
setup3

We already have:

A project
An environment setup in the project with datastore and workers
Set up the Python SDK
The Python SDK can be installed with pip from a terminal. And, for this use case, you also need NumPy.

pip install craft-ai-sdk numpy
Initialization of Python SDK
Copy and paste the following commands into a Python terminal or script to initialize the SDK:

from craft_ai_sdk import CraftAiSdk

sdk = CraftAiSdk(
    environment_url="MY_ENVIRONMENT_URL",
    sdk_token="MY_ACCESS_TOKEN"
)
Set the value of environment_url to your environment URL instead of MY_ENVIRONMENT_URL. You can find this URL on the Environments page in the UI. The Environment URL is NOT the address of the page in your browser.

Setup Step 6

Set the value of sdk_token to your access token instead of MY_ACCESS_TOKEN. On the UI, click on your name in the top right corner, then go to the â€œParametersâ€ page. There, you can generate and find your SDK token at the bottom. The SDK token is strictly personal. You must keep it confidential.

Setup Step 5

Finally, run your code.

[Recommended] Setup your credential with a .env file
Success

ðŸŽ‰ Well done! Youâ€™re ready to execute your first code on the platform!

What's next ?
Now that we have configured the platform, we can create our first objects and run a â€œHello Worldâ€ on the platform.

Next step: Part 1: Deploy a simple pipeline

Part 1: Execute a simple pipeline
The main goal of the Craft AI platform is to allow to deploy easily your machine learning pipelines.

In this part we will use the platform to build a simple â€œhello worldâ€ application by showing you how to execute a basic Python code that prints â€œHello worldâ€ and displays the number of days until 2025.

You will learn how to:

Package your application code into a step on the platform
Embed it in a pipeline
Execute it on the platform
Check the logs of the executions on the web interface
step1_00

Create a step with the SDK
The first thing to do to build an application on the Craft AI platform is to create a step.

A Step is the equivalent of a Python function in the Craft AI platform. Like a regular function, a step is defined by the inputs it ingests, the code it runs, and the outputs it returns. For this â€œhello worldâ€ use case, we are focusing on the code part so we will ignore inputs and outputs for now.

A step can be created from any Python function, using the create_step() method of thesdk object.

All of the code in this example can also be found on GitHub here.

For this example, we will use the following code:

import datetime

def helloWorld() -> None:

    # Count the number of days between January 1, 2000, and today
    start_date = datetime.datetime(2000, 1, 1)
    now = datetime.datetime.now()

    difference = now - start_date

    print(f'Hello world! Number of days since January 1, 2000: {difference.days}')
Create a file with the content above named src/part-1-helloWorld.py in a new folder that will contain the step's files. So, the helloWorld function is located in src/part-1-helloWorld.py.
We can now create the step by running the following command in a Python terminal:

sdk.create_step(
    step_name='part-1-hello-world',
    function_path='src/part-1-helloWorld.py',
    function_name='helloWorld',
    container_config={
        "local_folder": ".../get_started", # Enter the path to your local folder here, the one that contains `src/part-1-helloWorld.py`
    }
)
Its main arguments are:

The step_name is the name of the step that will be created. This is the identifier you will use later to refer to this step.
The function_path argument is the path of the Python module containing the function that you want to execute for this step. This path must be relative to the local_folder specified in the container_config.
The function_name argument is the name of the function that you want to execute for this step.
The container_config is the configuration of the container that will be used to execute the function.
Note

One of the container_config parameters is the local_folder parameter, which is the path to the folder we want to retrieve, containing the function to execute. We will explain in a later part how to do this differently, but for now, we focus on deploying steps from local code.

The above code should give you the following output:

>>> Please wait while step is being created. This may take a while...
>>> Steps creation succeeded
>>> {'name': 'part-1-hello-world'}
You can view the list of steps that you created in the platform with the list_steps() function of the SDK.

step_list = sdk.list_steps()
print(step_list)
>>> [{'step_name': 'part-1-hello-world',
>>>   'created_at': 'xxxx-xx-xxTxx:xx:xx.xxxZ',
>>>   'updated_at': 'xxxx-xx-xxTxx:xx:xx.xxxZ',
>>>   'status': 'Ready',
>>>   'origin': 'local'}]
You can see your step and its status of creation at Ready.

You can also get the information of a specific step with the get_step() function of the SDK.

step_info = sdk.get_step('part-1-hello-world')
print(step_info)
>>> {
>>>   'parameters': {
>>>     'step_name': 'part-1-hello-world',
>>>     'function_path': 'src/part-1-helloWorld.py',
>>>     'function_name': 'helloWorld',
>>>     'description': None,
>>>     'container_config': {
>>>       'language': 'python:3.X-slim',
>>>       'requirements_path': 'requirements.txt',
>>>       'dockerfile_path': None
>>>     },
>>>     'inputs': [],
>>>     'outputs': []
>>>   },
>>>   'creation_info': {
>>>     'created_at': 'xxxx-xx-xxTxx:xx:xx.xxxZ',
>>>     'updated_at': 'xxxx-xx-xxTxx:xx:xx.xxxZ',
>>>     'created_by': 'xxxxxxxx-xxxx-xxxx-xxxxx-xxxxxxxxxxx',
>>>     'updated_by': 'xxxxxxxx-xxxx-xxxx-xxxxx-xxxxxxxxxxx',
>>>     'status': 'Ready'
>>>     'origin': 'local',
>>>   }
>>> }
Success

ðŸŽ‰ Now your step has been created. You can now create your Pipeline (and after that, youâ€™ll execute it on the platform).

Create a pipeline with the SDK
step1_2

The step part-1-hello-world containing our helloWorld code is now created in the platform and ready to be used in a pipeline that we will then execute.

A pipeline is a machine learning workflow, consisting of one or more steps, that can be easily deployed on the Craft AI platform. This way, you can create a full pipeline formed with a directed acyclic graph (DAG) by specifying the output of one step as the input of another step.

In the future, it will be possible to assemble multiple steps into a complex machine learning pipeline. For now, the platform only allows single step pipelines.

To create a pipeline consisting of the previous step, you must use the create_pipeline() function of the SDK.

sdk.create_pipeline(
    pipeline_name='part-1-hello-world',
    step_name='part-1-hello-world',
)
This function has two arguments:

The pipeline_name is the name of the pipeline you have just created. As for the step_name you will then refer to the pipeline using this name
The step_name is the name of the step used in the pipeline.
After executing this function, you should see the following output :

>>> Pipeline creation succeeded
>>> {'pipeline_name': 'part-1-hello-world',
>>> 'created_at': 'xxxx-xx-xxTxx:xx:xx.xxxZ',
>>> 'steps': ['part-1-hello-world'],
>>> 'open_inputs': [],
>>> 'open_outputs': []}
Success

ðŸŽ‰ Now that our pipeline is created (around our step), we want to execute it. To do this, we will run the pipeline with the sdk function, run_pipeline(), and it will execute the code contained in the step.

Execute your pipeline (run)
You can execute a pipeline on the platform directly with the run_pipeline() function.

This function has two arguments:

The name of the existing pipeline to execute (pipeline_name)
Optional (only if you have inputs): a dict of inputs to pass to the pipeline with input names as dict keys and corresponding values as dict values.
sdk.run_pipeline(pipeline_name='part-1-hello-world')
>>> The pipeline execution may take a while, you can check its status and get information on the Executions page of the front-end.
>>> Its execution ID is 'part-1-hello-world-xxxxx'.
>>> Pipeline execution results retrieval succeeded
>>> Pipeline execution startup succeeded
Success

ðŸŽ‰ Now, you have created a step for the helloWorld function, included it in a pipeline and execute it on the platform! Our hello world application is built and ready to be executed again!

Get information about an execution
Now, we have executed the pipeline. The return of the function allows us to see that the pipeline has been successfully executed; however, it does not provide the logs of the execution (we can receive outputs with the return of the run pipeline, but we did not put any here).

To find the list of executions along with the information and associated logs, you can use the user interface as follows:

Connect to https://mlops-platform.craft.ai

Click on your project:

step1_3

Click on the Execution page and on â€œSelect an executionâ€: this displays the list of environments:

step1_4

Select your environment to get the list of runs and deployments:

step1_6

Finally, click on a run name to get its executions:

step1_7

You have the â€œGeneralâ€ tab to get general information about your execution and the â€œLogsâ€ tab where you can see and download the execution logs:

step1_8

Using the SDK
Success

ðŸŽ‰ You can now get your execution's logs.

What we have learned
In this part we learned how to easily build, deploy and use a simple application with the Craft AI platform with the following workflow:

step1_9

These 3 main steps are the fundamental workflow to work with the platform and we will see them over and over throughout this tutorial.

Now that we know how to run our code on the platform, it is time to create more complex steps to have a real ML use case.

Next step : Part 2: Execute a simple ML model


Part 2: Execute a simple ML model
The previous part showed the main concepts of the platform and how to use the basics of the SDK. With what you already know you are able to execute really simple pipelines. But in order to build more realistic applications, using more complex code on your data with dependencies such as Python libraries, it is needed to learn more advanced functionnalities and especially how to configure the execution context of a step and how to retrieve data stored on the platform.

This page will present the same commands as the previous ones going through more available functionalities offered by the platform, with a real Machine Learning use case. We will improve this Machine Learning application later in Part 3 and Part 4.

You can find all the code used in this part and its structure here.

Prerequisites

Python 3.8 or higher is required to be installed on your computer.
Have done the Part 1: Execute a simple pipeline.
Have scikit-learn, numPy and pandas libraries installed. If not, use these commands in your terminal:
pip install scikit-learn
pip install pandas
pip install numpy
Overview of the use case
We will build a pipeline to train and store a simple ML model with the iris dataset. The iris dataset describes four features (petal length, petal width, sepal length, sepal width) from three different types of irises (Setosa, Veriscolour, Virginica).

step2_0

The goal of our application is to classify flower type based on the previous four features. In this part we will start our new use case by retrieving the iris dataset from the Data Store (that we will introduce just below), building a pipeline to train a simple ML model on the dataset and store it on the Data Store.

step2_1

Storing data on the platform
The Data Store is a file storage on which you can upload and download unlimited files and organize them as you want using the SDK. All your steps can download and upload files from and to the Data Store.

Pushing the iris dataset to the Data Store: In our case the first thing we want to do is to upload the iris dataset to the Data Store. You can do so with the upload_data_store_object function from the SDK like so:

from io import BytesIO
from sklearn import datasets
import pandas as pd

iris = datasets.load_iris(as_frame=True)
iris_df = pd.concat([iris.data, iris.target], axis=1)

file_buffer = BytesIO(iris_df.to_parquet())
sdk.upload_data_store_object(
   filepath_or_buffer=file_buffer,
   object_path_in_datastore="get_started/dataset/iris.parquet"
)
The argument filepath_or_buffer can be a string or a file-like object. If a string, it is the path to the file to be uploaded, if a file-like object you have to pass an IO object (something you don't write to the disk but stay in the memory). Here we choose to use a BytesIO object.

Source code for model training
We will use the following code that trains a sklearn KNN classifier on the iris dataset from the Data Store and put the trained model on the Data Store.

import joblib
import numpy as np
import pandas as pd
from sklearn.neighbors import KNeighborsClassifier
from craft_ai_sdk import CraftAiSdk

def trainIris():

   sdk = CraftAiSdk()

   sdk.download_data_store_object(
      object_path_in_datastore="get_started/dataset/iris.parquet",
      filepath_or_buffer="iris.parquet",
   )
   dataset_df = pd.read_parquet("iris.parquet")

   X = dataset_df.loc[:, dataset_df.columns != "target"].values
   y = dataset_df.loc[:, "target"].values

   np.random.seed(0)
   indices = np.random.permutation(len(X))

   n_train_samples = int(0.8 * len(X))
   train_indices = indices[:n_train_samples]
   val_indices = indices[n_train_samples:]

   X_train = X[train_indices]
   y_train = y[train_indices]
   X_val = X[val_indices]
   y_val = y[val_indices]

   knn = KNeighborsClassifier()
   knn.fit(X_train, y_train)

   mean_accuracy = knn.score(X_val, y_val)
   print("Mean accuracy:", mean_accuracy)

   joblib.dump(knn, "iris_knn_model.joblib")

   sdk.upload_data_store_object(
      "iris_knn_model.joblib", "get_started/models/iris_knn_model.joblib"
   )
Delete objects
Before we really start building our new use case, we might want to clean the platform from the objects we created in Part 1. To do this, we need to use the functions associated with each object, here the pipeline and the step.

Warning

These objects have dependencies on each other, we have to delete them in a certain order. First the pipeline, then the step.

sdk.delete_pipeline(pipeline_name="part-1-hello-world")

sdk.delete_step(step_name="part-1-hello-world")
Advanced step configuration
In the rest of this part we will follow the same workflow as in the previous one:

step2_2

Now it is time to use the create_step() method of craft-ai-sdk object to create step like before. This time, we will define a bit more the step and its execution context. We are going to focus on two parameters.

Python libraries
As you might have noticed, the code above uses external Python libraries (craft_ai_sdk, joblib, numpy, pandas and scikit learn). In the previous step we built an application that didnâ€™t require any external dependency. This time if we want this code to work on the platform we have to inform it that this step requires some Python libraries to run properly.

To do so we create a requirements.txt file, containing the list of Python libraries used in our step function:

craft_ai_sdk==xx.xx.xx
joblib==xx.xx.xx
numpy==xx.xx.xx
scikit_learn==xx.xx.xx
pandas==xx.xx.xx
pyarrow==xx.xx.xx
Tip

Example with up-to-date version numbers available here.

In this case, we place it at the root of the folder with your code, but you can put it somewhere else.

You can set the default path for this file in the Libraries & Packages section of your project settings using the web interface. All steps created in this project will then use this path by default.

Success

ðŸŽ‰ Now all the steps created in this project will have the relevant libraries installed

Create a step
In the Project settings, you can see parameters that are applied by default when you create a step. Like the Python version you are using, information on your Git connection (if you want to use a Git repository instead of a local folder, more information here) and the libraries and packages installed on the project.

step2_4

By default, your step will apply those parameters during its creation. However, sometimes you want to define them only at the step level and override the default ones defined at the project level.

This is the role of the create_step() functionâ€™s container_config parameter. You can pass as a dictionary the set of parameters you want to use for the step creation. It allows you to be specific in the configuration of your step.

ðŸ’¡ For example, if you need to build a specific step with another version of Python, you can specify the new Python version at the step level using language in the container_config parameter.

Your project parameters will remain unchanged.

To go further
Here we will specify the requirements_path.

To take into account requirements.txt file, we must add it to the container_config parameter with the requirements_path.

sdk.create_step(
    step_name="part-2-iristrain",
    function_path="src/part-2-iris-train.py",
    function_name="trainIris",
    description="This function creates a classifier model for iris",
    container_config = {
        "requirements_path" : "requirements.txt", #put here the path to the requirements.txt
        "local_folder": ".../get_started", # Enter the path to your local folder here 
    }
)
It may also be useful to describe precisely the steps created to be able to understand their purpose afterward. To do so, you can fill in the description parameter during the step creation.

To go further with step creation

If you want to create a step based on the code of a Git repository, you can check this page.

Success

ðŸŽ‰ Now your step has been created. You can now create your Pipeline.

From here, we reproduce the same steps as before with the creation of the pipeline and we execute it.

Create a pipeline
Create a pipeline with the create_pipeline() method of the SDK.

sdk.create_pipeline(
    pipeline_name="part-2-iristrain",
        step_name="part-2-iristrain"
)
Execute your Pipeline and get the execution logs
Now you can execute your pipeline as in Part1.

sdk.run_pipeline(pipeline_name="part-2-iristrain")
You can find the list of executions with information and logs in the frontend on the Execution tracking page.

The output is a list of iris categories :

>> [2 0 2 0 2 2 0 0 2 0 0 0 1 2 2 0 0 0 1 1 0 0 1 0 2 1 2 1 0 2 0 2 0 0 2 0 2
1 1 1 2 2 2 1 0 1 2 2 0 1 1 2 1 0 0 0 2 1 2 0]
Success

ðŸŽ‰ You can now execute a more realistic Machine Learning pipeline.

Now that we can have more complex code in our steps and we know how to parametrize the execution context of our steps, we would like to be able to give it input elements to vary the result and receive the result easily. For this, we can use the input/output feature offered by the platform.

Next step: Part 3: Execute with input and output


Part 3: Execute a ML use case with inputs and outputs
In Part 2, we have built and run our first ML pipeline to retrieve data from the data store, train a model and store it on the data store.

We will now train our model with new data, by adding an Input to the pipeline and send the predictions to a final user, by adding an Output to the pipeline.

We will first create the code of the predictIris() function so that it can receive data and return predictions.
Then, we will see how to create a step, a pipeline and run it on the platform with input data and return the corresponding predictions as an output.
By the end of this part, we will have built a runable pipeline that allows to get the predictions of the iris species on new data with a simple execution.

Prerequisites

Python 3.8 or higher is required to be installed on your computer.
Have done the previous parts of this tutorial (Part 1: Execute a simple pipeline and Part 2: Execute a simple ML model).
Overview of the use case
We will build a pipeline to retrieve the trained model stored in the last part and make a prediction on new data.

step3_0

The code we want to execute
First we have to implement our code to compute predictions with a stored model on the data store on any (correctly prepared) data given as input instead of computing predictions on a test set. Hence, our file src/part-3-iris-predict.py is as follows:

from io import BytesIO
from craft_ai_sdk import CraftAiSdk
import joblib
import pandas as pd


def predictIris(input_data: dict, input_model_path:str):

   sdk = CraftAiSdk()

   f = BytesIO()
   sdk.download_data_store_object(input_model_path, f)
   model = joblib.load(f)

   input_dataframe = pd.DataFrame.from_dict(input_data, orient="index")
   predictions = model.predict(input_dataframe)

   final_predictions = predictions.tolist()

   return {"predictions": final_predictions}
In this code:

We add the argument input_data. Here, we choose it to be a dictionary like the one below:
{
    1: {
        'sepal length (cm)': 6.7,
      'sepal width (cm)': 3.3,
      'petal length (cm)': 5.7,
      'petal width (cm)': 2.1
    },
  2: {
      'sepal length (cm)': 4.5,
      'sepal width (cm)': 2.3,
      'petal length (cm)': 1.3,
      'petal width (cm)': 0.3
  },
}
It contains the data on which we want to compute predictions.

We retrieve our trained model with the download_data_store_object() function of the sdk by passing the model path.

At the end, we convert our input_data dictionary into a Pandas dataframe, and we compute predictions with our trained model.

As you can see, the function now returns a Python dict with one field called â€œpredictionsâ€ that contains the predictions value. The platform only accepts step function with one return value of type ``dict``. Each item of this dict will be an output of the step and the key associated with each item will be the name of this output on the platform.

Moreover, you can see that we converted our result from a numpy ndarray to a list. That is because the values of the inputs and outputs are restricted to native Python types such as int, float, bool, string, list and dict with elements of those types. More precisely anything that is json-serializable. Later, the platform might handle more complex input and output types such as numpy array or even pandas dataframe.

Dont forget to update your requirements.txt file, containing the list of Python libraries used in our step function:

joblib==xx.xx.xx
pandas==xx.xx.xx
craft_ai_sdk==xx.xx.xx
Tip

Example with up-to-date version numbers available here.

Step creation with Input and Output
Now, letâ€™s create our step on the platform. Here, since we have inputs and an output, our step is the combination of three elements: an input, an output and the Python function above. We will first declare the inputs and the output. Then, we will use the function sdk.create_step() as in Part 2 to create the whole step.

step3_1

Declare Input and Output for a new step
To manage inputs and outputs of a step, the platform requires you to declare them using the Input and Output classes from the SDK.

For our Iris application, the inputs and outputs declaration would look like this:

from craft_ai_sdk.io import Input, Output

prediction_input = Input(
   name="input_data",
   data_type="json"
)

model_input = Input(
   name="input_model_path",
   data_type="string"
)

prediction_output = Output(
   name="predictions",
   data_type="json"
)
Both objects have two main attributes:

The name of the Input or Output

For the inputs it corresponds to the names of the arguments of your stepâ€™s function. In our case name="input_data" and "input_model_path", as in the first line of function:

def predictIris(input_data: dict, input_model_path:str):
For the output it must be a key in the dictionary returned by your stepâ€™s function. In our case, name="predictions" as in the last line of function:

return {"predictions": final_predictions}
The data_type describes the type of data it can accept. It can be one of: string, number, boolean, json, array, file.

For the inputs we want a dictionary and a string as we specified, which corresponds to data_type="json" and data_type="string".

For the output, we return a dictionary which corresponds to data_type="json".
Now, we have everything we need to create the step and the pipeline corresponding to our predictIris() function.

Create step
Now as in Part 2, we'll create our step on the platform using the sdk.create_step() function, but this time we specify our inputs and output:

sdk.create_step(
   step_name="part-3-irisio",
   function_path="src/part-3-iris-predict.py",
   function_name="predictIris",
   description="This function retrieves the trained model and classifies the input data by returning the prediction.",
   inputs=[prediction_input, model_input],
   outputs=[prediction_output],
   container_config={
     "local_folder": ".../get_started", # Enter the path to your local folder here 
     "requirements_path": "requirements.txt",
   },
)
This is exactly like in part 2, except for two parameters:

inputs containing the list of Input objects we declared above (here, prediction_input and model_input).
outputs containing the list of Output objects we declared above (here, prediction_output).
When step creation is finished, you obtain a return describing your step (including its inputs and outputs) as below:

>> Step "part-3-irisio" created
  Inputs:
    - input_data (json)
    - input_model_path (string)
  Outputs:
    - predictions (json)
>> Steps creation succeeded
>> {'name': 'part-3-irisio',
 'inputs': [{'name': 'input_data', 'data_type': 'json'}, {'name': 'input_model_path', 'data_type': 'string'}],
 'outputs': [{'name': 'predictions', 'data_type': 'json'}]}
Now that our step is created in the platform, we can embed it in a pipeline and run it.

Create pipeline
Letâ€™s create our pipeline here with sdk.create_pipeline() as in Part 2:

sdk.create_pipeline(
   pipeline_name="part-3-irisio",
   step_name="part-3-irisio",
)
You quickly obtain this output, which describes the pipeline, its step and its inputs and outputs:

>> Pipeline creation succeeded
>> {'pipeline_name': 'part-3-irisio',
 'created_at': 'xxxx-xx-xxTxx:xx:xx.xxxZ',
 'steps': ['part-3-irisio'],
 'open_inputs': [{'input_name': 'input_data',
   'step_name': 'part-3-irisio',
   'data_type': 'json'}, {'input_name': 'input_model_path',
   'step_name': 'part-3-irisio',
   'data_type': 'string'}],
 'open_outputs': [{'output_name': 'predictions',
   'step_name': 'part-3-irisio',
   'data_type': 'json'}]}
Success

ðŸŽ‰ Youâ€™ve created your first step & pipeline with inputs and outputs!

Letâ€™s run this pipeline.

Run a pipeline with new input data
Prepare input data
Now, our pipeline needs data as input (formatted as we said above â¬†ï¸). Letâ€™s prepare it, simply by choosing some of the rows of iris dataset we did not use when training our model:

import numpy as np
import pandas as pd
from sklearn import datasets

np.random.seed(0)
indices = np.random.permutation(150)
iris_X, iris_y = datasets.load_iris(return_X_y=True, as_frame=True)
iris_X_test = iris_X.loc[indices[90:120],:]

new_data = iris_X_test.to_dict(orient="index")
Letâ€™s check the data we created:

print(new_data)
We get the following output:

>> 124: {'sepal length (cm)': 6.7,
'sepal width (cm)': 3.3,
'petal length (cm)': 5.7,
'petal width (cm)': 2.1
},
41: {'sepal length (cm)': 4.5
...
Finally, we need to encapsulate this dictionary in another one whose key is "input_data" (the name of the input of our step, i.e. the name of the argument of our stepâ€™s function). We define also the path to our trained model on the data store with the value associated to the key "input_model_path".

inputs = {
    "input_data": new_data,
    "input_model_path": "get_started/models/iris_knn_model.joblib"
}
In particular, when your step has several inputs, this dictionary should have as many keys as the number of inputs the step have.

Execute the pipeline (RUN)
Finally, we can execute our pipeline with the data weâ€™ve just prepared by calling the run_pipeline() function almost as in Part 2 and passing our dictionary inputs to the inputs arguments of the function:

output_predictions = sdk.run_pipeline(
                        pipeline_name="part-3-irisio",
                        inputs=inputs)
Finally, our output can be obtained like this:

print(output_predictions["outputs"]['predictions'])
This gives the output we want (with the predictions!):

>> {'predictions': [0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,2]}
Moreover, you can check the logs on the web interface, by clicking on the Executions tracking tab of your environment, selecting your pipeline and choosing the last execution.

Success

ðŸŽ‰ Congratulations! You have run a pipeline to which we can pass new data, the path to our trained model and get predictions.

Next step: Part 4: Deploy a ML use case with inputs and outputs


Part 4: Deploy a ML use case with inputs and outputs
Introduction
In Part 3, we have built and run our second ML pipeline to retrieve our trained model from the data store, provide some new data to it as input and retrieve the result as an output of our pipeline execution.

What if we want to let an external user execute our predict pipeline? Or if we want to schedule the execution of the pipeline that trains our model periodically?

â‡’ We need to deploy one pipeline via an endpoint and another one with a scheduled execution.

This part will show you how to do this with the Craft AI platform:

We will first update the code of the predictIris() function so that it can retrieve directly from the data store the trained model and returns the predictions as a json to the user.
We will also update the code of the trainIris() function so that it re trains the model on a specific dataset (that could be often updated) and uploads the trained model directly to the datastore.
Then, we will see how to create a step and a pipeline that we will deploy on the platform in two different ways, and that could be executed periodicly and by a call.
Prerequisites

Python 3.8 or higher is required to be installed on your computer.
Have done the previous parts of this tutorial ( Part 0: Setup, Part 1: Execute a simple pipeline, Part 2: Execute a simple ML model, Part 3: Execute a ML use case with inputs and outputs).
Machine Learning application with I/O
Here we will build an application based on what we did on the last part. We will expose our service to external users and schedule periodic executions.

Overview of the use case
To get the predictions via and endpoint:
step4_0

To retrain the model periodicly (we will focus on this case later):
step4_0_bis

The code we want to execute
We will first focus on the construction of the endpoint the final user will be able to target.

First we have to update our code to retrieve directly the model from the data store without any call to the sdk in the code and to return a file on the data store with the predictions inside. Hence, our file src/part-4-iris-predict.py is as follows:

import joblib
import pandas as pd
import json

def predictIris(input_data: dict, input_model:dict):

   model = joblib.load(input_model['path'])

   input_dataframe = pd.DataFrame.from_dict(input_data, orient="index")
   predictions = model.predict(input_dataframe)

   return {"predictions": predictions.tolist()}
What changed are only how we get the trained model.

model = joblib.load(input_model['path'])
input_model is a dictionary in which the key path refers to the file's path where is located the file on the step environnement.

This input is a file data type.

Don't forget to update your requirements.txt file, containing the list of Python libraries used in our step function:

joblib==xx.xx.xx
pandas==xx.xx.xx
Tip

Example with up-to-date version numbers available here.

Step creation with Input and Output
As we did in part 3, we will first declare the inputs and the output. Then, we will use the function sdk.create_step() to create the whole step.

step4_1

Declare Input and Output of our new step
The only difference now is the data type we will assign to input_model. This is now a file that we want to retrieve from the data store. To do so, we define the inputs and output like below:

from craft_ai_sdk.io import Input, Output

prediction_input = Input(
   name="input_data",
   data_type="json"
)

model_input = Input(
   name="input_model",
   data_type="file"
)

prediction_output = Output(
   name="predictions",
   data_type="json"
)
We have just seen the code of the step has been adapted to handle file objects.

Now, we have everything we need to create, as before, the step and the pipeline corresponding to our predictIris() function.

Create your step
Now as in Part 3, it is time to create our step on the platform using the sdk.create_step() function, with our inputs and output:

sdk.create_step(
   step_name="part-4-iris-deployment",
   function_path="src/part-4-iris-predict.py",
   function_name="predictIris",
   description="This function retrieves the trained model and classifies the input data by returning the prediction.",
   inputs=[prediction_input, model_input],
   outputs=[prediction_output],
   container_config={
     "local_folder": ".../get_started", # Enter the path to your local folder here 
     "requirements_path": "requirements.txt",
   },
)
When the step creation is finished, you obtain an output describing your step (including its inputs and outputs) as below:

>> Step "part-4-iris-deployment" created
  Inputs:
    - input_data (json)
    - input_model (file)
  Outputs:
    - predictions (json)
>> Steps creation succeeded
>> {'name': 'part-4-iris-deployment',
 'inputs': [{'name': 'input_data', 'data_type': 'json'}, {'name': 'input_model', 'data_type': 'file'}],
 'outputs': [{'name': 'predictions', 'data_type': 'json'}]}
Now that our step is created in the platform, we can embed it in a pipeline and deploy it.

Create your pipeline
Letâ€™s create our pipeline here with sdk.create_pipeline() as in Part 3:

sdk.create_pipeline(
   pipeline_name="part-4-iris-deployment",
   step_name="part-4-iris-deployment",
)
You quickly obtain this output, which describes the pipeline, its step and its inputs and outputs:

>> Pipeline creation succeeded
>> {'pipeline_name': 'part-4-iris-deployment',
 'created_at': 'xxxx-xx-xxTxx:xx:xx.xxxZ',
 'steps': ['part-4-iris-deployment'],
 'open_inputs': [{'input_name': 'input_data',
   'step_name': 'part-4-iris-deployment',
   'data_type': 'json'}, {'input_name': 'input_model',
   'step_name': 'part-4-iris-deployment',
   'data_type': 'file'}],
 'open_outputs': [{'output_name': 'predictions',
   'step_name': 'part-4-iris-deployment',
   'data_type': 'json'}]}
Success

ðŸŽ‰ Youâ€™ve created your second step & pipeline with inputs and output!

Create your deployments with input and output mappings
Here, we want to be able to execute the pipeline, either by launching the execution with an url link or at a certain time, but not by a run anymore.

Let's try the first case.

We want the user to be able to:

send the input data directly to the application via an url link
retrieve the results directly from the endpoint
We want also to specify the path to the stored model on the data store, so that the service will take this model directly from the data store. The user won't be the one selecting the model used, it's only on the technical side.

Create the endpoint with IO mappings
An endpoint is a publicly accessible URL that launches the execution of the Pipeline.

Without the platform, you would need to write an api with a library like Flask, Fast API or Django and deploy it on a server that you would have to maintain.

step4_2

Create the endpoint
To create an endpoint, go to the UI. Once in your project, go to the Pipelines page and select your environment. On this page select your pipeline part-4-iris-deployment and press Deploy.

On the page, enter the name for your deployment, (here you will use part-4-iris-deployment) and select Elastic mode in Execution Mode and Endpoint in Execution Rule.

step4_6

Once you have done this, click on 'Next Step' where you will be presented with the mapping page to fill in.

IO Mappings
For each input and output in your pipeline, you need to define the source or destination:

Input input_data: Select Endpoint. You can leave the default name in Exposed name.
Input input_model: Select Datastore and enter the path to the file get_started/models/iris_knn_model.joblib in the Datastore.
Output predictions: Select Endpoint and iris_type as the exposed name.
step4_7

Then click Create Deployment and the build will begin.

Target the endpoint
Prepare the input data

Now, our endpoint needs data as input, like we did for last part:

import numpy as np
import pandas as pd
from sklearn import datasets

np.random.seed(0)
indices = np.random.permutation(150)
iris_X, iris_y = datasets.load_iris(return_X_y=True, as_frame=True)
iris_X_test = iris_X.loc[indices[90:120],:]

new_data = iris_X_test.to_dict(orient="index")
We need to encapsulate this dictionary in another one whose key is "input_data" (the name of the input of our step, i.e. the name of the argument of our stepâ€™s function). We don't need to define the path to our trained model because it is already defined with the output mapping we have just done.

inputs = {
    "input_data": new_data
}
Call the endpoint with the input data

endpoint_url = sdk.base_environment_url  + "/endpoints/part-4-iris-deployment"
endpoint_token = "MY_ENDPOINT_TOKEN"
request = requests.post(endpoint_url, headers={"Authorization": f"EndpointToken {endpoint_token}"}, json=inputs)
request.json()
Warning

Don't forget to include your deployment name (in the URL) and your endpoint token. All this information is available in the deployment information, as shown here:

Step 4.5

There is also sample code you can copy-paste to call the endpoint.

The HTTP code 200 indicates that the request has been taken into account. In case of an error, we can expect an error code starting with 4XX or 5XX.

It is a way to call your deployment. But, obviously, you can call it with any other HTTP client (curl command in bash, Postmanâ€¦).

Warning

Note that you can't directly call your endpoint and receive the output by entering the URL in your web browser, as the request is based on the POST method and requires an authentication header.

Let's check we can get the predictions as output of the endpoint:

print(request.json()['outputs']['iris_type'])
Moreover, you can check the logs on the UI, by clicking on the Executions tab of your environment, selecting your pipeline and choosing the last execution.

Success

ðŸŽ‰ Youâ€™ve created your first deployment and you've just called it!

Retrain the model periodically
Let's imagine that our dataset is frequently updated, for instance we get new labeled iris data every day. In this case we might want to retrain our model by triggering our training pipeline part4-iris-train every day.

The platform can do this automatically using the periodic execution rule in our deployment.

A periodic execution rule allows to schedule a pipeline execution at a certain time. For example, every Monday at a certain time, every month, every 5 minutes etc.

The inputs and output have to be defined, with a constant value or a data store mappings. First we will update our trainIris function so that it produces a file output containing our model, that we will then map to the datastore. You can check the entire updated version of this function in src/part-4-iris-predict.py. The only change is done at the return of the function:

> return {"model": {"path": "iris_knn_model.joblib"}}
We can then create the step and pipeline as we are used to.

train_output = Output(
   name="model",
   data_type="file"
)

sdk.create_step(
   step_name="part-4-iristrain",
   function_path="src/part-4-iris-predict.py",
   function_name="trainIris",   
   container_config={
        "local_folder": ".../get_started", # Enter the path to your local folder here 
   },
   outputs=[train_output],
)

sdk.create_pipeline(
   pipeline_name="part-4-iristrain",
   step_name="part-4-iristrain"
)
Now let's create a deployment that executes our pipeline every 5 minutes (In a real case, we would have set it to every day. However, for this example, let's set it to every 5 minutes, so we can see it in action immediately). In our case, we will map the prediction output (which is our only I/O) to the datastore on the same path that is used in the pediction endpoint deployment. This way our prediction pipeline will automatically use the latest version of our model for predictions.

step4_3

Create periodic deployment

Let's create a deployment that will schedule our pipeline to be executed every 5 minutes. IO mappings will be the same as in the endpoint, except that this time the input data is a constant and not something that is provided to the endpoint whenever it is called.

Let's return to the Pipelines page and deploy our pipeline. Select Elastic for the Execution Mode and Periodic for the Execution Rule. We then need to add a trigger (at the bottom of the page) and specify that the execution should be repeated every 5 minutes.

step4_8

Once the trigger is created you can go to the Next step.

Adapt IO mapping

Then, you need to define the destination of your output:

Output model: Select Datastore and enter the path to the file get_started/models/iris_knn_model.joblib in the datastore.
step4_10

And now, you can complete the deployment of your pipeline.

Our training pipeline will now be executed every 5 minutes, updating our model with the potential new data. The predict pipeline will then use this updated model automatically.

You can check that you actually have a new execution every 5 minutes using the sdk or via the web interface.

Success

ðŸŽ‰ Congrats! Youâ€™ve created your second deployment and planned it to run every 5 minutes!

Conclusion
Success

ðŸŽ‰ After this Get Started, you have learned how to use the basic functionalities of the platform! You know now the entire workflow to create a pipeline and deploy it.

step4_4

You are now able to:

Deploy your code through a pipeline in a few lines of code, run it whenever you want and have the logs to analyze the execution.
Use the Data Store on the platform to upload and download files, models, images, etc.
Execute your pipeline via an endpoint that is accessible from outside with a secured token, or via a periodic execution.
Make your inputs flexible: set constant values to avoid users to fill in, let users enter inputs values via the endpoint directly, or use the data store to retrieve or put objects.
If you want to go further

One concept has not been explained to you: the metrics.

If you want to go further and discover this feature, you can read the associated documentation.



Accessing your data
When executing your code within Pipelines on the Craft AI Platform, one crucial aspect is the ability to retrieve and store data. In this context, we will explore two approaches to acquiring data during our Pipeline Executions:

Using the Data Store provided by the Craft AI Platform, which offers a convenient way to store and retrieve objects on the environment.
Connecting to external data sources like you are used to. By accessing your own organization database, cloud external database, some open source data, data available via FTP or some data storage such as the classic ones offered by AWS, Azure, or Google Cloud Platform, we can expand the range of data available for our Pipeline Executions.
By combining the capabilities of both the Data Store and external data sources, we can ensure a reliable and efficient data retrieval system for our executions. We will take a closer look at the techniques and practices of retrieving data for pipeline executions.

Warning

Be aware that when an execution is launched on the platform, what is on its execution context is not persistent (i.e. it does not remain after the execution), all data kept in memory or on the disk is removed at the end of the execution. It is possible to read, write and manipulate data during the execution, but everything in the execution context at the end of the execution is deleted.

This allows you to have an identical and stable execution context for each run, while avoiding needlessly saturating the disk with your executions.

To ensure the persistence of your data, you can use data sources: It can be the Data Store, your own database or external data storage.

Summary
How to store data on the Data Store
How to retrieve data from the Data Store
How to access an external Data Source
How to store data on the Data Store
Here we will see how to store files created within a step.

To do this, we will see 2 methods:

With the dedicated SDK function upload_data_store_object()

Advantages:

More flexibility at the Data Store path level. This method allows you to upload files to any location on the Data Store, as you can easily change the file path by providing a different input to the code.
Drawbacks:

Need to initialize the SDK in the step.
No tracking (the file path values given as inputs are not stored).
With an Output mapping to the Data Store

Advantages: - No need to initialize the SDK on the step. - tracking of the file used in this execution.

Drawbacks:

Need to define the Storage location on the Dataupload_data_store_object() Store before creating the deployment and the path canâ€™t be modified afterwards.
Only one possible storage location.
With the dedicated SDK function upload_data_store_object()
You can also access the Data Store directly from the step code by using the SDK function upload_data_store_object().

Warning

Note that you need to import the craft_ai_sdk library at the beginning of the code, and have it in your requirements.txt.

The SDK connection is initialized without token or environment URL, since the step will already be executed in the environment.

Example

get_data.py
from craft_ai_sdk import CraftAiSdk

def preprocess_data():

    # SDK initialization
    sdk = CraftAiSdk()

    # Using an existing function get_data() to get raw data
    df_data = get_data()
    # Using an existing function preprocess_data() to preprocess
    # the previously retrieved data
    df_preprocessed_data = preprocess_data(df_data)
    # The existing function create_csv()  writes the dataframe 
    # df_preprocessed_data as a csv in path_to_preprocessed_data
    create_csv(df_preprocessed_data, path_to_preprocessed_data)

    # Upload into datastore with the SDK function upload_data_store_object()
    sdk.upload_data_store_object(
        filepath_or_buffer=path_to_preprocessed_data, 
        object_path_in_datastore="data/preprocessed/data_preprocessed.txt"
    )
Then, simply create the step and the pipeline to execute this code.

With an Output mapping to the Data Store
Here we will see how to store files that were created within a step to the data store.

To do so, we have a few points to follow:

Adapt the code that will be executed in the step, especially by specifying the path on which the file is accessible on the step.
def yourFunction() :
    ...
    return {"outputfile" : 
        {"path": **path of the file on the step**}
    }
Before creating the step, define the output of the step with a data_type as file, and create the step and the pipeline as we are used to.
from craft_ai_sdk.io import Output

step_output = Output(
    name=step_output_name,
    data_type="file", 
)

sdk.create_step(
    function_path=**the path to your function**,
    function_name="my_function_name", 
    step_name=**my step name**,
    container_config = { 
        "local_folder": "my_step_folder/",
    },

    outputs=[step_output]
)

sdk.create_pipeline(pipeline_name=**your pipeline name**, 
    step_name=**your step name**)
Define the output mapping (with the sdk object OutputDestination) that will be used for the execution of your pipeline, in this case, a datastore_path is needed to map the output of the step to a specific path on the Data Store.
from craft_ai_sdk.io import OutputDestination

output_mapping = OutputDestination(
    step_output_name=step_output_name,
    datastore_path=**path on which we want to store the file**,
)

sdk.run_pipeline(
    pipeline_name=**your pipeline name**,
    outputs_mapping=[output_mapping]
)
Example

In this part, we will create a deployment with an endpoint execution rule that returns 2 files, 1 file with text in .txt format and another with a fake confusion matrix in format .csv.

First, we write the code to create the 2 files.To be able to pass the files to the Data Store, we specify the paths of the 2 files on the step.

Note

Note the step execution context is an isolated container, and it's the platform that will then copy the files from the execution context to the Data Store (thanks to the output weâ€™ve created).

Donâ€™t forget to indicate the dependencies into requirements.txt.

import numpy as np
import pandas as pd

def createFiles() :

    # Define file into local step contenaire 
    path_text = "file_text_output.txt"
    path_matrix = "confusion_matrix.csv"

    # Create a fake confusion matrix into .csv file
    confusion_matrix = np.array([[100, 20, 5],
                                [30, 150, 10],
                                [10, 5, 200]])
    class_labels = ['Class A', 'Class B', 'Class C'] # Define the class labels
    df = pd.DataFrame(confusion_matrix, index=class_labels, columns=class_labels) # Create a DataFrame from the confusion matrix
    df.to_csv(path_matrix, index=True, header=True) # Save the DataFrame as a CSV file

    # Create .txt file
    text_file = open(path_text, 'wb')  # Open the file in binary mode
    text_file.write("Result of step send in file output :) ".encode('utf-8'))  # Encode the string to bytes
    text_file.close()

    # Return the path of the file in the container of the current step execution.
    fileOjb = {
        "txtFile" : {"path": path_text}, 
        "csvFile" : {"path": path_matrix}
    }
    return fileOjb
Warning

Remember to push step code to a GitHub repository defined in information project into Craft AI platform.

After the initialization of SDK connection, we can create the 2 outputs and then the step.

For this step, we assume that all the information is already specified in the project settings (language and repository information).

# Output creation
step_output_txt = Output(
    name="txtFile",
    data_type="file", 
)

step_output_csv = Output(
    name="csvFile",
    data_type="file", 
)

# Step creation with output (we supose repository is setup in info project)
sdk.create_step(
    function_path="src/createFiles.py",
    function_name="createFiles", 
    step_name="doc-2o-datastore-step",
    container_config = { 
        "local_folder": "my_step_folder/",
    },

    outputs=[step_output_txt, step_output_csv]
)
Now, we can create the pipeline.

sdk.create_pipeline(pipeline_name="doc-2o-datastore-pipeline", 
    step_name="doc-2o-datastore-step")
To create an accessible endpoint, we need to create a deployment with two output mappings to the Data Store we created earlier.

Letâ€™s pretend we want to store the files at "docExample/resultText.txtâ€ and "docExample/resultMatrix.csv" on the Data Store.

endpoint_output_txt = OutputDestination(
    step_output_name="txtFile",
    datastore_path="docExample/resultText.txt",
)

endpoint_output_csv = OutputDestination(
    step_output_name="csvFile",
    datastore_path="docExample/resultMatrix.csv",
)

endpoint = sdk.create_deployment(
    execution_rule="endpoint",
    pipeline_name="doc-2o-datastore-pipeline",
    deployment_name="doc-2o-datastore-dep",
    outputs_mapping=[output_mapping_txt, output_mapping_csv]
)
After that, we can trigger the endpoint with the Python code below (we can use any other tool like postman, curl â€¦).

import requests 

endpoint_url = **your-url-env**+"/endpoints/doc-2o-datastore-dep"
headers = {"Authorization": "EndpointToken "+endpoint["endpoint_token"]}

response = requests.post(endpoint_url, headers=headers)

print (response.status_code, response.json())
How to retrieve data from the Data Store
Here, we will see how to retrieve files already stored on the Data Store within a step.

To do this, we will see 2 methods:

With the dedicated SDK function download_data_store_object()

Advantages: - More flexibility at the Data Store path level. This method allows you to download files from any location on the Data Store, as you can easily change the file path by providing a different input to the code.

Drawbacks:

Need to initialize the SDK in the step.
No tracking (the file path values given as inputs are not stored).
With an Input mapping to the Data Store
Advantages: - No need to initialize the SDK on the step. - tracking of the file used in this execution.

Drawbacks:

Need to define the Storage location on the Data Store before creating the deployment and the path canâ€™t be modified afterwards.
Only one possible storage location.
With the dedicated SDK function download_data_store_object()
You can access the Data Store directly from the step code by using the SDK function download_data_store_object().

Note

Note that you need to import the craft_ai_sdk library at the beginning of the code, and have it in your requirements.txt.

The connection is initialized without token or environment URL, since the Step will already be executed in the environment.

Example

from craft_ai_sdk import CraftAiSdk

def retrievePredictions(id_prediction: int):

    # SDK initialization
    sdk = CraftAiSdk()  

    # Download the file containing the predictions 
    # of the id_prediction at the local path "predictions.txt"
    sdk.download_data_store_object(
        object_path_in_datastore=f"data/predictions_{id_prediction}.csv", 
        filepath_or_buffer="predictions.txt"
    )

        # Open and print the content of the file now stored locally
    with open("predictions.txt") as f:
        contents = f.readlines()
        print (contents)
Then, simply create the step and the pipeline to execute this code.

With an Input mapping to the Data Store
We will need to define the Input as a file for the step, the input mapping that connects the Data Store to the step and therefore the code embedded within the step to correctly read the file. Letâ€™s start with the latter.

To do so, we have a few points to follow:

Adapt the code to access and read your file. Indeed, the input that will be passed to the step will have a predefined form as a dictionary with path as key and the file path as a value. You thus need to access it the same way you retrieve the value of a dictionary. The file will be downloaded in the execution environment before the step is executed. You can then use the file as you would use any other file in the execution environment.

Here, we have a function readFile that aims to read the input file and print its content.

def read_file (entryFile: dict) :

    # Access the file with its local path (entryFile["path"]) on the step
    with open(entryFile["path"]) as f:
        contents = f.readlines()
        print (contents)
Warning

One the code is updated, remember to push step code to a GitHub repository defined in information project into Craft AI platform.

Before creating the step, define the input of the step with a data_type as file, and create the step and pipeline as we are used to.

Note

We assume that all the information is already specified in the project settings (language, repository and branch information).

from craft_ai_sdk.io import Input

# Define the input of the step 
step_input = Input(
    name="entryFile",
    data_type="file",    
)

# Create the step 
sdk.create_step(
    function_path="src/read_file.py",
    function_name="read_file", 
    step_name="file-datastore-step",
    container_config = { 
        "local_folder": "my_step_folder/",
    },

    inputs=[step_input]
)

# Create the pipeline
sdk.create_pipeline(pipeline_name="file-datastore-pipeline", 
    step_name="file-datastore-step")
Once the pipeline is created, we define the correct input mapping and run the pipeline with it.

Define the input mapping (with the SDK object InputSource) that will be used for the execution of your pipeline, in this case, a datastore_path is needed to map the input of the step to the file on the Data Store.

Letâ€™s pretend the file we want to retrieve is stored at myFolder/text.txt on the Data Store.

from craft_ai_sdk.io import InputSource

input_mapping = InputSource(
    step_input_name="entryFile"
    datastore_path="myFolder/text.txt", # Path of the output file in the datastore
)

# Run pipeline using mapping defined with InputSource object
sdk.run_pipeline(
    pipeline_name="file-datastore-pipeline", 
    inputs_mapping=[input_mapping]
)
You can check the logs for your file content.

pipeline_executions = sdk.list_pipeline_executions(
    pipeline_name="file-datastore-pipeline"
)

logs = sdk.get_pipeline_execution_logs(
    pipeline_name=pipeline_name, 
    execution_id=pipeline_executions[0]['execution_id']
)

print('\n'.join(log["message"] for log in logs))
How to access an external Data Source
The connection with an external data source (database or data storage) involves the following steps:

Using the same code you would use without the Craft AI platform to access your data storage. You only have to encapsulate your code within a step and a pipeline, like any code you would like to execute on the Craft AI platform. You also may have to adapt the inputs and outputs of your main function to respect the Craft AI formats.

Embedding your credentials on your platform environment to use them in your step code to access the database.

ðŸŒŸ To securely use credentials, a common good practice is to define environment variables to store the credentials safely. If you want to do so, the SDK offers you an easy solution: the create_or_update_environment_variable() function.

sdk.create_or_update_environment_variable("USERNAME", "username_db_1")
Then, when you need the credentials, you access it with the following command :

import os
import dotenv

dotenv.load_dotenv()

username = os.environ["USERNAME"]
From an external Database
Moreover, if you try to access to an external Database, we may have to whitelist the IP of your Craft AI platform environment(s) if necessary in your data source configuration. The environments IP are available on the page dedicated to your project environments.

env_info

Here is an example of the few points to do in order to access a specific external database directly from our Craft AI environment platform by using the usual credentials and be able to filter on some data on the database:

Tip

If needed, add the Craft AI environment platform URL to the whitelist of the external database you want to access.

First, we embed the credentials we usually use to access to the database in our environment by setting them as environment variables.
sdk.create_or_update_environment_variable(
    environment_variable_name="DB_HOST",
    environment_variable_value="xxx")

sdk.create_or_update_environment_variable(
    environment_variable_name="DB_ADMIN",
    environment_variable_value="xxx")

sdk.create_or_update_environment_variable(
    environment_variable_name="DB_PASS",
    environment_variable_value="xxx")

sdk.create_or_update_environment_variable(
    environment_variable_name="DB_NAME",
    environment_variable_value="xxx")

sdk.create_or_update_environment_variable(
    environment_variable_name="DB_PORT",
    environment_variable_value="xxx")
Secondly, we encapsulate the code we would use without the Craft AI platform in a function, here filter_data_from_database(), that will be executed within a step:

Below, we use an existing function create_db_connection() that creates a connection to the database with the credentials brought on our Craft AI environment platform.
Then, we use an existing function filter_data() that retrieves some data in a specific table on the database by filtering with the ids inputs on a specific column.
As input of our filter_data_from_database() function, we have this list of integer, ids, and as output we have a dictionary whose key is filtered_data. ids and filtered_data are respectively input and output of the step we would define after.
Here is the corresponding code:

import os
import dotenv

dotenv.load_dotenv()

DB_HOST = os.environ["DB_HOST"]
DB_ADMIN = os.environ["DB_ADMIN"]
DB_PASS = os.environ["DB_PASS"]
DB_NAME = os.environ["DB_NAME"]
DB_PORT = os.environ["DB_PORT"]

def filter_data_from_database(ids: List[int]) :
    try:
        # With an existing function create_db_connection,
        # Creating a connection to the database with the 
        # credentials brought on our Craft AI environment platform.
        conn = create_db_connection(
            host=DB_HOST,
            user=DB_ADMIN,
            password=DB_PASS,
            database=DB_NAME,
            port=DB_PORT
        )

        # With an existing function filter_data, 
        # Retrieving some data in a specific table on the database 
        # by filtering with the ids inputs on a specific column.
        df_data_filtered = filter_data(conn, ids)
        df_data_filtered_final = df_data_filtered.tolist()
    finally:
        conn.close()
    return {"filtered_data": df_data_filtered_final}
From an external Data Storage
Here is an example of the few points to do in order to access a specific external data storage directly from a Craft AI environment platform by using the usual credentials and be able to retrieve one specific csv file:

First, we embed the credentials we usually use to access to the data storage in our environment by setting them as environment variables.
sdk = CraftAiSdk(
    sdk_token=**our-sdk-token**, 
    environment_url=**our-environment-url**)

sdk.create_or_update_environment_variable(
    environment_variable_name="SERVER_PUBLIC_KEY",
    environment_variable_value="xxx")

sdk.create_or_update_environment_variable(
    environment_variable_name="SERVER_SECRET_KEY",
    environment_variable_value="xxx")

sdk.create_or_update_environment_variable(
    environment_variable_name="REGION_NAME",
    environment_variable_value="xxx")
Secondly, we encapsulate the code we would use without the Craft AI platform in a function that will be executed within a step.
Below, we use an existing function configure_client() that configures a client (data storage specific) with the credentials brought on our Craft AI environment platform to access the data storage.
Then, we use an existing function get_object_from_bucket() that, with the configured client, retrieves the object key in the bucket bucket on the data storage.
import os
import dotenv

dotenv.load_dotenv()

SERVER_PUBLIC_KEY = os.environ["SERVER_PUBLIC_KEY"]
SERVER_SECRET_KEY = os.environ["SERVER_SECRET_KEY"]
REGION_NAME = os.environ["REGION_NAME"]

def filter_data_from_data_storage(bucket: str, key:str)

    # With an existing function configure_client, 
    # Configuring a client (data storage specific) 
    # with the credentials brought on our Craft AI environment platform.
    client = configure_client(
        public_key=SERVER_PUBLIC_KEY, 
        secret_key=SERVER_SECRET_KEY, 
        region=REGION_NAME)

    # With an existing function get_object_from_bucket,
    # Retrieving the object key in the bucket bucket on the data storage.
    buffer = get_object_from_bucket(
        client=client,
        bucket_name=bucket,
        key=key
    )

    dataframe = pd.read_csv(buffer)
    return dataframe


Adapt your code
You have an existing function executing locally or on a docker for example, you would like to execute it on the Craft AI platform.

Whether it be a very basic function, one with inputs and/or outputs, or one that uses external data, it may be necessary to adapt the code to make it runnable without error on the platform.

ðŸ’¡ Best practice, have code ready for production

Functional Application Design:
Structure your production code into nested functions to promote modularity, reuse, and I/O tracking.
Do not use notebooks for production; use Python scripts and associated functions.
Identify the main inputs and desired outputs of the main function.

Application Evolution Management:

Use Git to version your code, monitor development, and track changes throughout the application's lifecycle.
Commit and push your code when you're ready to release it.

Explicit Dependencies for Stability:

Choose and declare the right dependencies in requirements.txt to ensure constant functionality.
Specify the version of Python compatible with your project during commissioning (available directly in the platform, project parameter).
Prerequisites
Before using the Craft AI platform, make sure you have completed the following prerequisites:

Get access to an environment
Connect the SDK

If this is not the case, you can go to this page for more information : Connect to the platform

How can I Encapsulate a basic script ?
Before explaining how to encapsulate, let's first look at what encapsulation is in the Craft AI platform. Encapsulation is the act of taking your python code and putting it into a standard object for the platform that allows it to run with all the specific features it needs.

For example, encapsulation allows you to :

Have all pip libraries correctly installed
Receive and send data with the platform and your users
Etc.
When your script is encapsulated in the platform, it is called a â€œstepâ€.

The aim of the platform is to enable you to create a step from your code with as little change as possible to your source code. Some adaptations may therefore be necessary, and we'll explain them here.

Encapsulate â€œHello worldâ€ case
The platform encapsulates Python scripts in order to execute them. For reasons of ease of use (particularly with regard to input/output), the platform encapsulates Python functions directly.

Let's imagine a hello_world.py script:

âŒ this script does not work

print ("Hello world")
âœ… this script works

def fct_hello(): 
    print ("Hello world")
Once you have uploaded your script to the repository linked to your environment. You can use the Python SDK to request encapsulation of your script with just 1 line of code:

sdk.create_step(
    function_path="hello_world.py", # Path of were is the script file in your repo
    function_name="fct_hello", # Name of function to run
    step_name="step_example" # Unique name to identify your step in your environment
    container_config = { 
        "local_folder": "my_step_folder/",
    }
)
Finally, when your step is ready, you can create a pipeline from it that will be ready to be executed at any time by the platform.

Encapsulate a script that uses pip to install libraries
In data science projects, it is common to use libraries not native to Python, generally installed with pip. As part of the encapsulation process, this must also be specified when the step is created using a requirement.txt file.

To do this, add the requirement.txt file to the repository where the script to be executed in the pipeline is located, then specify it when the step is created.

sdk.create_step(
    function_path="hello_world.py", # Path of were is the script file in your repo
    function_name="fct_hello", # Name of function to run
    step_name="step_example" # Unique name to identify your step in your environment

    # arg requirements_path should be in dict in container_config parameter.
    container_config = { 
        "requirements_path": "requirements.txt",    # requirement path in repository
    "local_folder": "my_step_folder/",
    },
)
Info

In addition to pip dependencies, you can add Linux-compatible system dependencies (equivalent to an apt-get install).

Note that this is cumulative with pip dependencies.


Example:

```py
sdk.create_step(
    function_path="hello_world.py", # Path of were is the script file in your repo
    function_name="fct_hello", # Name of function to run
    step_name="step_example" # Unique name to identify your step in your environment

    # arg requirements_path should be in dict in container_config parameter
    container_config = { 
        "system_dependencies": ["python3-opencv", "python3-scipy"],# list of dependancy name
    "local_folder": "my_step_folder/",
    },
)
Encapsulate a script that uses other existing functions of my repository
The selected folders/files are copied from the repository to the step. The default selection is defined in the project parameters. If necessary, this can be changed to include other files from the repository or, on the contrary, to deselect them. The 2 conditions are :

The python script must be part of the selected folders
The content of all the selected folders must be less than 5MB.
Example:

sdk.create_step(
    function_path="hello_world.py", # Path of were is the script file in your repo
    function_name="fct_hello", # Name of function to run
    step_name="step_example" # Unique name to identify your step in your environment

    container_config = { 
        # import folder "/src"  and file "asset/data.csv" from repo to step
        "included_folders": ["src", "asset/data.csv"] ,
    "local_folder": "my_step_folder/",
    },
)
How I encapsulate a script with input and output ?
When I encapsulate the function in my script, it can request values as input parameters and return values as output with return. How do I interact between these elements and the platform?

To do this, we use the input and output system. These are elements that allow us to:

Transmit data from the platform to the function parameters using inputs.
Transmit the data returned by the function to the platform using outputs.
Note that the inputs and outputs of a step are defined when it is created (see example below).

Info

To tell the platform where to fetch data for inputs and where to send data for outputs, we use mappings that we define when we deploy the pipeline. For more information, click here.

Which data types are available ?
Here is a list of possible types for inputs and outputs:

array
boolean
json
number
string
file
Note that the input and output system automatically converts data into objects that can be used in Python. For example, JSON become Python dict, arrays become Python lists, etc.

Info

The files work a little differently, and if you need to access data from external Data Sources, more information here.

Example :

In this example, we define two inputs of type number and JSON and an output of type array. The step can be represented as follows:

step_schema_workflow

Info

In this diagram, the arrows with "Adapt process" represent the automatic conversion of types by the platform mentioned earlier.

Our source code, which will be contained in a function with two parameters and which returns a list in its return. Here is an example of python code in a script.py file:

def my_function(int_value, dictionary):
    # Print the parameters
    print("Parameter int:", int_value)
    print("Parameter dict:", dictionary)

    # Create the array (list) using the values from the dictionary
    result = list(dictionary.values())
    result.append(int_value)

    # Return the resulting array
    return result
Once the script.py file has been sent to the repository, you can run the python code that uses the Craft AI SDK to create the inputs, outputs, and step:

# Import and init craft AI SDK before 

step_input1 = Input(
  name="int_value", # same name as 1er parameter mandatory
    data_type="number",
)

step_input2 = Input(
  name="dictionary",# same name as 2nd parameter mandatory
    data_type="json",
)

step_output = Output(
  name="result",
    data_type="array"
)

sdk.create_step(
    function_path="script.py",
    function_name="my_function", 
    step_name="step_A",
    container_config={
      "local_folder": "my_step_folder/",
    },

    inputs=[step_input1, step_input2],
    outputs=[step_output]
)
Warning

Beware, for now all existing data types are not available yet (for example pd.DataFrame, pd.Series, etc.). You have to make sure you adapt your functionâ€™s code to be compatible.

To do so, you can choose from various available IO types.

How to have a default input value ?
In order for certain step code to function correctly, it is possible that certain input values are mandatory in order to launch execution. There are 2 solutions for this:

is_required: Used to make the input value mandatory in order to launch execution. If this is the case, execution will return an error without executing the code.
default_value: Used to give a default value to the input when the basic value used is empty.
Info

If the value of an input is required and has a default value, then the default value will be used for execution in the event of a missing value.

Example :

For this example, we'll use the source code of the step we defined in the previous example. However, we're going to modify the code calling the Craft AI platform to notify it :

A default value for int_value.
That the dictionary input is mandatory for executing the step
# Import and init craft AI SDK before 

step_input1 = Input(
  name="int_value", # same name as 1er parameter mandatory
    data_type="number",
    default_value=12,
)

step_input2 = Input(
  name="dictionary",# same name as 2nd parameter mandatory
    data_type="json",
    is_required=True,
)

step_output = Output(
  name="result",
    data_type="array"
)

sdk.create_step(
    function_path="script.py",
    function_name="my_function", 
    step_name="step_A",
    container_config={
      "local_folder": "my_step_folder/",
    },

    inputs=[step_input1, step_input2],
    outputs=[step_output]
)
Full example
In this example, we will adapt an ML application (image classification) with :

Python dependencies
System dependencies
Two string inputs (the access path to the image and the model in the data store)
One string output (the classification result)
Python application source code :

import cv2
import numpy as np
from tensorflow.keras.models import load_model

def perform_inference(image_path, model_path):
    # Load the pre-trained model
    model = load_model(model_path)

    # Read the image using OpenCV
    image = cv2.imread(image_path)

    # Preprocess the image (you may need to adjust this based on your model requirements)
    # Example: Resize the image to the input size expected by the model
    input_size = (224, 224)
    preprocessed_image = cv2.resize(image, input_size)
    preprocessed_image = preprocessed_image / 255.0  # Normalize pixel values to [0, 1]

    # Perform inference using the loaded model
    prediction = model.predict(np.expand_dims(preprocessed_image, axis=0))

    # Replace this with your post-processing logic based on the model output
    # For example, if it's a classification model, you might want to get the class with the highest probability
    predicted_class = np.argmax(prediction)

    return predicted_class

# Example usage
image_path = 'path/to/your/image.jpg'
model_path = 'path/to/your/model.h5'
result = perform_inference(image_path, model_path)

print(f"Predicted Class: {result}")
Requirements.txt :

numpy
tensorflow
If you want to test this script locally as if it were run in a step by the platform, you could use a bash script like this:

Bash command

# Install OpenCV using apt-get
# Note: This assumes you are using a Debian-based system
sudo apt-get update
sudo apt-get install -y python3-opencv

# Upgrade pip and install Python dependencies from requirements.txt
pip install --upgrade pip
pip install -r requirements.txt

# Run app.py
python app.py
Or its equivalent with a Dockerfile :

Dockerfile

# Use an official Python image as a parent image
FROM python:3.9-slim

# Set the working directory to /app
WORKDIR /app

# Copy the current directory contents into the container at /app
COPY . /app

# Install OpenCV using apt-get
RUN apt-get update && apt-get install -y \
    python3-opencv

# Install any Python dependencies from requirements.txt
RUN pip install --upgrade pip && \
    pip install -r requirements.txt

# Make port 80 available to the world outside this container
EXPOSE 80

# Run app.py when the container launches
CMD ["python", "app.py"]
Once the script app.py and the requirements.txt file are on the Git repository, all you have to do is use the Craft AI SDK to define the inputs and outputs, and create the step with the correct parameters.

# Import and init craft AI SDK before 

step_input1 = Input(
  name="image_path", 
    data_type="string",
    is_required=True,
)

step_input2 = Input(
  name="model_path",
    data_type="string",
    is_required=True,
)

step_output = Output(
  name="result",
    data_type="string"
)

sdk.create_step(
  function_path="app.py",
  function_name="perform_inference", 
  step_name="step_B",
    container_config = { 
    "requirements_path": "requirements.txt",
    "system_dependencies": ["python3-opencv"],
    "local_folder": "my_step_folder/",
  },
  inputs=[step_input1, step_input2],
  outputs=[step_output]
)
The values provided to the Python script during function calls (see code below) will be supplied at each execution and can be modified by the user calling the step. More details here.


Run and serve your pipelines
Data scientists have a crucial role in the creation and implementation of machine learning models, converting advanced algorithms into useful applications. In tasks like data processing, training, scheduling, and model deployment, data scientists often face the challenge of initiating pipelines using different methods while efficiently handling various inputs. This adaptability is vital because it allows them to address a wide range of scenarios and demands.

To streamline this process, the MLOps platform provides a robust and user-friendly deployment features, empowering data scientists to execute and serve their pipelines efficiently.

Learn about the various ways to execute your pipelines, which include:

Pipeline run
Pipeline deployment, with 2 execution rules
Endpoint rule
Periodic rule
One of the fundamental elements provided by the MLOps platform to run and serve your models is the ability to customize the sources of the inputs you provide to a pipeline and the ways you want to retrieve your outputs. The main idea is to easily connect different data sources to your inputs (data directly coming from the final user, or coming from the environment for example) and destinations (delivering output to the final user, writing it in the data store ...). This is known as mapping in the platform, and it plays a central role when creating deployments or running pipelines.

Summary
How to execute your pipeline
How to define sources and destinations for step inputs and outputs ?
Prerequisites
Before using the Craft AI platform, make sure you have completed the following prerequisites:

Get access to an environment
Connect the SDK
Create a step
Create a pipeline
Info

Make sure that your code inside your step with inputs/outputs is able to read inputs from function parameters and send outputs with the return of the function. More information here.

How to execute your pipeline
As a data scientist, I want to be able to execute my Python code contained in my pipelines in various scenarios :

I (or data scientist in my team) want to launch my pipeline on the fly via the craft AI SDK, so I can use the run pipeline.
I want to be able to make my pipeline accessible to external users, inside or outside my organization, via a Application Programmatic Interface (API), I can use the endpoint deployment feature to accomplish this.
I want my pipeline to be automatically execute at regular intervals without manual intervention, the periodic deployment feature can be employed.
Run your pipeline
The run pipeline functionality enables you to trigger a pipeline directly from the Craft AI platform's SDK. This option offers the advantage of being user-friendly and efficient.

Please note that the run pipeline feature is exclusively accessible through the Craft AI SDK and requires proper connection to the appropriate platform environment. It is recommended for conducting experiments and conducting internal testing purposes.

To execute a run on a pipeline with no input and no output, simply use this function:

# Run pipeline function 
sdk.run_pipeline(pipeline_name="your-pipeline-name")
To execute a run on a pipeline with inputs and outputs, you can use the same function and add the parameter inputs and give an object with inputs' names as keys (should be the same name as defined in input object give at the step creation) and values you want to provide to your step at execution:

# Creating an object with predefined input values for the run,
# which will be provided as inputs to the run pipeline function
inputs_values = {
    "number1": 9, # Value for Input "number1" of the step (defined during step creation)
    "number2": 6 # Value for Input "number2" of the step (defined during step creation) 
}

# Running the Pipeline and Receiving Output
output_values = sdk.run_pipeline(pipeline_name="your-pipeline-name", inputs=inputs_values)

print (outputs_values["outputs"])
For example, you can obtain an output like this :

>> {
    'output_name_1': 'Lorem ipsum dolor sit amet, consectetur adipiscing elit.',
    'output_name_2': 42
 }
This function will return you the output of your pipeline. Like the inputs, itâ€™s represented as an object with the outputs' name as keys and the outputs values as values.

Warning

Donâ€™t forget to adapt your code to get value from input and return output. You must have created inputs and outputs objects at the step creation stage to get values. More information here.

It is also possible to run a pipeline with specific mapping (to connect inputs/outputs to the data store, environment variable, etc.), that will be covered in this section.

Create an endpoint
Triggering a pipeline via an endpoint will enable you to make your application available from any programming language/tool (website, mobile application, etc.).

To make your pipeline available via an endpoint, we need to create a deployment using the create_deployment() function which has the execution_rule parameter set as endpoint. In return, we'll get the URL of the endpoint and its authentication token so that we can call it and trigger our pipeline.

# Deployment creation with endpoint as execution rule
endpoint_info = sdk.create_deployment(
    execution_rule="endpoint",
    pipeline_name="your-pipeline-name",
    deployment_name="your-deployment-name",
)
Note

Here, we focus mainly on the platform's SDK interface. However, it is possible to deploy a pipeline directly from the web interface by going to the Pipelines page, selecting a pipeline and then clicking the Deploy button.

You can trigger this endpoint from anywhere with any programming language if you have:

Environment URL (Can be found in web UI and itâ€™s the same you have used to initiate your SDK)
Deployment name
Endpoint token (given as result of the deployment creation, which secures access to the endpoint)
To trigger the deployed pipeline as an endpoint, you have a couple of options:

Utilize the dedicated function provided by the Craft AI SDK.

# Execution of pipeline using the endpoint
sdk.trigger_endpoint(
    endpoint_name="your-deployment-name", 
    endpoint_token=endpoint_info["endpoint_token"] # Token recieve after deployment creation 
)
Implement a HTTP request to the endpoint using any programming language, similar to the example shown with the command curl :

curl -X POST -H "Authorization: EndpointToken <ENDPOINT_TOKEN>" "<CRAFT_AI_ENVIRONMENT_URL>/endpoints/<DEPLOYMENT_NAME>"
An other example with Javascript syntax (using axios) :

const axios = require('axios');

const ENDPOINT_TOKEN = '<ENDPOINT_TOKEN>';
const CRAFT_AI_ENVIRONMENT_URL = '<CRAFT_AI_ENVIRONMENT_URL>';
const DEPLOYMENT_NAME = '<DEPLOYMENT_NAME>';

const headers = {
  'Authorization': `EndpointToken ${ENDPOINT_TOKEN}`
};

const url = `${CRAFT_AI_ENVIRONMENT_URL}/endpoints/${DEPLOYMENT_NAME}`;

axios.post(url, null, { headers })
  .then(response => {
    console.log('Response:', response.data);
  })
  .catch(error => {
    console.error('Error:', error.message);
  });
Info

The URL of your endpoint follows the structure : <CRAFT_AI_ENVIRONMENT_URL>/endpoints/<YOUR_DEPLOYMENT_NAME>

The inputs and outputs defined in your step are automatically linked to the deployment and consequently to the endpoint. You can therefore send a JSON within your request where the parameters correspond to the step inputs so that it can be used as a parameter for your function in the step. The deployment will return a similar object with your outputs as parameters.

You can call the endpoint with inputs using the SDK function or with any other programming language (like before) by specifying the inputs in JSON format in the request body.

With Craft AI SDK :

# Value of inputs to be given to trigger pipeline function
inputs_values = {
    "number1": 9, # Input "number1" defined in step creation 
    "number2": 6 # Input "number2" defined in step creation 
}

# Execution of pipeline using the endpoint
sdk.trigger_endpoint(
    endpoint_name="your-deployment-name", 
    endpoint_token=endpoint_info["endpoint_token"] # Token recieve after deployment creation 
    inputs=inputs_values
)
With curl :

curl -X POST -H "Authorization: EndpointToken <ENDPOINT_TOKEN>" -H "Content-Type: application/json" -d '{"number1": 9, "number2": 6}' "<CRAFT_AI_ENVIRONMENT_URL>/endpoints/<YOUR-DEPLOYMENT-NAME>"
With JavaScript using axios :

const axios = require('axios');

const ENDPOINT_TOKEN = '<ENDPOINT_TOKEN>';
const CRAFT_AI_ENVIRONMENT_URL = '<CRAFT_AI_ENVIRONMENT_URL>';
const YOUR_DEPLOYMENT_NAME = '<YOUR-DEPLOYMENT-NAME>';

const requestData = {
  number1: 9,
  number2: 6
};

const config = {
  headers: {
    'Authorization': `EndpointToken ${ENDPOINT_TOKEN}`,
    'Content-Type': 'application/json'
  }
};

const url = `${CRAFT_AI_ENVIRONMENT_URL}/endpoints/${YOUR_DEPLOYMENT_NAME}`;

axios.post(url, requestData, config)
  .then(response => {
    console.log('Response:', response.data);
  })
  .catch(error => {
    console.error('Error:', error);
  });
Info

For inputs and outputs, don't forget to have adapted the step code, to have declared the inputs and outputs at step level and to have used types (string, integer, etc.) compatible with the data you are going to manipulate.

Periodic
You might need to trigger your Python code regularly, whether it's every X minutes, every hour, or at a specific date, automatically. To achieve this, you can create a periodic deployment for your pipeline. This type of deployment uses the CRON format for scheduling its triggers.

To set up a periodic deployment, you employ the same function as you would for endpoints. However, you specify the periodic trigger mode and define when it should trigger by providing a CRON rule in the schedule parameter.

deployment_info = sdk.create_deployment(
    execution_rule="periodic",
    pipeline_name="your-pipeline-name",
    deployment_name="your-deployment-name",
    schedule="* * * * *" # Will be executed every minute of every day 
)
More information and help about CRON here.

How to define sources and destinations for step inputs and outputs ?
In the previous section, we saw that deployments can be used to trigger the execution of a pipeline, but they can also be used to give and receive information via inputs and outputs. When a pipeline is triggered, it may need to receive or send this information to different sources or destinations.

Example:

In the diagram below, we assume that we have deployed an endpoint pipeline. An API is therefore available to the user who triggers the execution of the pipeline each time a request is sent to the API. The request can contain information required to execute the pipeline, just as the pipeline can send information back to the user, as shown in the diagram below.

schema_sourcedest

To direct these flows to the right place, the platform allows you to map the step inputs and outputs to different sources and destinations using InputSource and OutputDestination objects. We'll look at four different types of mapping:

Constant mapping
Endpoint mapping (value or file)
Environment variable mapping
None / void mapping
Inputs and outputs are not compatible with all types of deployment. To make things clearer, here is a summary table:

Constant	Endpoint value	Endpoint file	Environment variable	Data store file	None / void
Run	âœ…	âŒ	âŒ	âœ… (input only)	âœ…	âœ…
Endpoint	âœ…	âœ…	âœ… (limited to 1 file per call)	âœ… (input only)	âœ…	âœ…
Periodic	âœ…	âŒ	âŒ	âœ… (input only)	âœ…	âœ…
Constant
If I want my deployment to always use the same value as input, I can use a mapping to a constant.

Info

Note that the same pipeline can have multiple deployments with multiple different constant values for the same input.

We will be using the constant value for an endpoint deployment here, but the process is the same for other types of deployment (periodic).

To do this, we create an InputSource object for each input in the pipeline that we want to deploy, specifying the name of the input for each mapping and the value of the input using the constant_value parameter.

Example :

# Endpoint Input mapping
endpoint_input1 = InputSource(
    step_input_name="number1",
    constant_value=6,
)

endpoint_input2 = InputSource(
    step_input_name="number2",
    constant_value=3,
)

# Deployment creation using inputSource object 
endpoint_info = sdk.create_deployment(
    execution_rule="endpoint",
    pipeline_name="your-pipeline-name",
    deployment_name="your-deployment-name",
    inputs_mapping=[endpoint_input1, endpoint_input2]
)
Warning

Step must be created with one or more inputs and the code contained in my step must be suitable for receiving a constant. More information about it here.

All input types are compatible, except for file.

Endpoint
As explained above, when a deployment is of the endpoint type, the inputs and outputs of the associated steps have as their default source and destination the parameters of the HTTP request.

You can therefore send a JSON within your request where the keys correspond to the step inputs so that it can be used as parameters for your function in the step. The deployment will return a similar object with your outputs as keys.

If you need to change this default behavior, you can do so using InputSource and OutputDestination. You can define new names that will only be seen at the endpoint level for the external user. It allows you to specify under which names should inputs be passed in the request JSON by your final user or for the outputs, under which names they will be returned.

This input mapping works with any type of input or output (integer, file, string, etc.).

# Endpoint Input mapping
endpoint_input1 = InputSource(
    step_input_name="number1",
    endpoint_input_name="number_a",
)

endpoint_input2 = InputSource(
    step_input_name="number2",
    endpoint_input_name="number_b",
) 

endpoint_output = OutputDestination(
    step_output_name="number3",
    endpoint_output_name="result",
)

# Deployment creation using 2 input mapping and 1 ouptut mapping
endpoint_info = sdk.create_deployment(
    execution_rule="endpoint",
    pipeline_name="your-pipeline-name",
    deployment_name="your-deployment-name",
    inputs_mapping=[endpoint_input1, endpoint_input2],
    outputs_mapping=[endpoint_output]
)
Now that we have created our deployment, we can trigger the pipeline through the endpoint by passing the inputs and reading the outputs with the new mappings.

# Value of inputs to be given to trigger pipeline function
inputs_values = {
    "number_a": 9, # Using mapping defined before linked to number1
    "number_b": 6 # Using mapping defined before linked to number2
}

# Execution of pipeline using the endpoint
endpointOutput = sdk.trigger_endpoint(
    endpoint_name="your-deployment-name", 
    endpoint_token=endpoint_info["endpoint_token"] # Token received after deployment creation 
    inputs=inputs_values
)

# Print result, note you should go into "outputs" before
print (endpointOutput["outputs"]["result"])
These changes are effective for any endpoint trigger method (curl, JS, etc.).

Info

Obviously, this type of mapping is only available if the deployment is an endpoint.

Environment variable
As a data scientist, I may need to use data common for my entire environment in my pipeline. This can be achieved by mapping environment variables to inputs.

To do this, we also use the mapping system with the InputSource object. Environment variables are only available for inputs and not for outputs (but it is still possible to define them directly in the step code).

First, let's look at how to initialize the two environment variables (on the platform environment), we're going to use:

# Creation of env variable for input1
sdk.create_or_update_environment_variable(
    environment_variable_name="RECETTE_VAR_ENV_INPUT1",
    environment_variable_value=6
)

# Creation of env variable for input2
sdk.create_or_update_environment_variable(
    environment_variable_name="RECETTE_VAR_ENV_INPUT2",
    environment_variable_value=4
)
Now, we can create an InputSource object and use it at the deployment creation.

# Creation of InputSource to get env variable into input 
endpoint_input1 = InputSource(
     step_input_name="number1",
     environment_variable_name="RECETTE_VAR_ENV_INPUT1",
)

endpoint_input2 = InputSource(
     step_input_name="number2",
     environment_variable_name="RECETTE_VAR_ENV_INPUT2",
)

# Endpoint creation
endpoint_info = sdk.create_deployment(
    execution_rule="endpoint",
    pipeline_name="your-pipeline-name",
    deployment_name="your-deployment-name",
    inputs_mapping=[endpoint_input1, endpoint_input2]
)
The deployment is created, we can trigger it without giving any object into data of HTTP request, it will take current environment variable value.

# Value of inputs to be given to execute pipeline function
inputs_value = {
    "number_a": 9, # Using mapping defined before linked to number1
    "number_b": 6 # Using mapping defined before linked to number2
}

# Execution of pipeline using the endpoint
sdk.trigger_endpoint(
    endpoint_name="your-deployment-name", 
    endpoint_token=endpoint_info["endpoint_token"] # Token recieve after deployment creation 
    inputs=inputs_value
)
Data store
You may need to transfer files between your data store and your step (in one direction or the other). To do this, you can also use inputs and outputs mapping system. You'll find all the explanations you need on this page.

Void / None
All step inputs and outputs have to be mapped when you deploy a pipeline with inputs and outputs (otherwise it will raise an error when creating the deployment). If you don't really want to give/receive data in these inputs and outputs, you can map them to None. This will create a None object in Python for the inputs and send the outputs into the void.

# Endpoint Input mapping
endpoint_input1 = InputSource(
    step_input_name="number1",
    is_null=True,
)

endpoint_input2 = InputSource(
    step_input_name="number2",
    is_null=True,
) 

endpoint_output = OutputDestination(
     step_output_name="number3",
     is_null=True
)

# Deployment creation using 2 input mappings and 1 ouptut mapping
endpoint_info = sdk.create_deployment(
    execution_rule="endpoint",
    pipeline_name="your-pipeline-name",
    deployment_name="your-deployment-name",
    inputs_mapping=[endpoint_input1, endpoint_input2],
    outputs_mapping=[endpoint_output]
)
Now that the deployment has been created, it can be triggered without giving or receiving any input or output.

# Execution of pipeline using the endpoint who will return any output (object with value)
sdk.trigger_endpoint(
    endpoint_name="your-deployment-name", 
    endpoint_token=endpoint_info["endpoint_token"] # Token received after deployment creation
)


Understanding ML executions
As data scientists, the ability to comprehensively understand and monitor machine learning executions is paramount for delivering successful and impactful models.

This page is a resource that equips data scientists with the necessary knowledge and tools to track, monitor, and analyze the executions of their machine learning models on the Craft AI platform.

This page delves into essential topics, including obtaining execution details, tracking input and output, accessing metrics and logs, and comparing multiple executions. By mastering these techniques, data scientists can make informed decisions, optimize their models, and unlock the true potential of the MLOps platform.

Info

On this page, we will focus more on the pages available in the web interface. We'll mention the SDK functions available without going into detail about them.

Topics

How to find an execution and obtain the details ?
How to compare multiple execution ?
How to follow a pipeline in production ?
Prerequisites
Before using the Craft AI platform, make sure you have completed the following prerequisites:

Get access to an environment
Connect the SDK
Create a step & pipeline
Execute this pipeline
Warning

The tensorflow library does not work with Python 3.12 and later versions yet. We strongly recommend you to use an older version of Python (such as 3.11) to complete these examples.

On this page, we're going to focus on execution analysis using the platform's web interface. For the example, I'm going to use a basic deep learning use case to visualize the associated information. You can use your own use cases, of course.

Step code used in parts 1 and 2 (model training):

import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.datasets import fashion_mnist
from craft_ai_sdk import CraftAiSdk

def trainer(learning_rate=0.001, nb_epoch=5, batch_size=64) : 

    sdk = CraftAiSdk()

    # Load and preprocess the MNIST dataset
    (train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()
    train_images = train_images.reshape((60000, 28, 28, 1)).astype('float32') / 255
    test_images = test_images.reshape((10000, 28, 28, 1)).astype('float32') / 255
    train_labels = tf.keras.utils.to_categorical(train_labels)
    test_labels = tf.keras.utils.to_categorical(test_labels)

    # Build the neural network model
    model = models.Sequential()
    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))
    model.add(layers.MaxPooling2D((2, 2)))
    model.add(layers.Conv2D(64, (3, 3), activation='relu'))
    model.add(layers.MaxPooling2D((2, 2)))
    model.add(layers.Conv2D(64, (3, 3), activation='relu'))
    model.add(layers.Flatten())
    model.add(layers.Dense(64, activation='relu'))
    model.add(layers.Dense(10, activation='softmax'))

    # Compile the model
    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),
                loss='categorical_crossentropy',
                metrics=['accuracy'])

    # Train the model
    for epoch in range(nb_epoch):  # Set the number of epochs
        model.fit(train_images, train_labels, epochs=1, batch_size=batch_size, validation_split=0.2)

        # Evaluate the model on the test set and log test metrics
        test_loss, test_acc = model.evaluate(test_images, test_labels)
        print({'epoch': epoch + 1, 'test_loss': test_loss, 'test_accuracy': test_acc})
        sdk.record_list_metric_values("test-loss",  test_loss)
        sdk.record_list_metric_values("test-accuracy", test_acc)


    # Evaluate the model on the test set
    test_loss, test_acc = model.evaluate(test_images, test_labels)
    print(f'Test accuracy: {test_acc}')
    sdk.record_metric_value("accuracy", test_acc)
    sdk.record_metric_value("loss", test_loss)

    # Save the model
    model.save('mnist-model.keras')
    sdk.upload_data_store_object('mnist-model.keras', 'product-doc/mnist-model.keras')
Step code used in part 3 (inference with the model):

import tensorflow as tf
from tensorflow.keras.models import load_model
from PIL import Image
import numpy as np
from craft_ai_sdk import CraftAiSdk
import tensorflow as tf
from tensorflow.keras.datasets import fashion_mnist

def inference(image_num, model_path):

    sdk = CraftAiSdk()
    ima_path = './image_from_fashion_mnist.jpg'

    # Load the Fashion MNIST dataset
    (train_images, train_labels), (vali_images, vali_labels) = fashion_mnist.load_data()

    # save 1 image of validation dataset 
    image_to_save = Image.fromarray(vali_images[image_num]) 
    image_to_save.save(ima_path)    


    # Save model in local context and load it
    sdk.download_data_store_object(model_path, "model.keras")
    model = load_model("model.keras")


    # Preprocess the input image
    input_image = tf.keras.preprocessing.image.load_img(ima_path, target_size=(28, 28), color_mode='grayscale')
    input_image = tf.keras.preprocessing.image.img_to_array(input_image)
    input_image = np.expand_dims(input_image, axis=0)
    input_image = input_image.astype('float32') / 255.0

    # Make predictions
    predictions = model.predict(input_image)

    # The predictions are probabilities, convert them to class labels
    predicted_class = int(np.argmax(predictions[0]))

    if vali_labels[image_num] : 
        if vali_labels[image_num] == predicted_class:
            sdk.record_metric_value("score", 1)
        else :
            sdk.record_metric_value("score", 0)

    return {"predicted_class": str(predicted_class)}
How to find an execution and obtain the details ?
When using the platform for experimentation or production, you can find the list of all your executions on the Execution > Execution Tracking page (remember to select a project first).

On this page you will find the list of all executions in all environments of the selected project. All executions are listed, whether in progress, failed or finished, whether triggered by a run, endpoint or CRON.

Warning

Please note that deleting the pipeline or deployment will delete all attached executions.

Get general information on an execution
Once you are in the execution tracking page, you need to select an environment using the selector at the top left. Once the mouse is over the environment, you will see another popup on the right with two lists in a row:

The first contains the list of pipelines that have 'run' attached to them.
The second contains the list of deployments that have executed attached to them.
Once a pipeline or deployment has been selected, the list of executions appears in the left-hand column, from the most recent to the oldest.

Tip

You can click on an environment directly to get all the associated executions.

understand_ML_exec_1

Info

You can also retrieve all this information using the sdk.get_pipeline_execution(execution_id) function via the SDK.

Track input and output of execution
If you want to see the inputs and outputs of a execution, you can view them in the tab of the same name. The inputs/outputs of the pipeline are displayed in a table with their:

Name
Type
Source/destination type (where the value entered for this execution comes from)
Source/destination value (what is the value entered for this execution)
understand_ML_exec_2

Info

For the SDK, this information can be obtained using the function mentioned above sdk.get_pipeline_execution(execution_id).

More information can be obtained using the:

sdk.get_pipeline_execution_input(execution_id, input_name)
sdk.get_pipeline_execution_output(execution_id, output_name)
Get metrics and logs of execution
In the metrics tab, you can retrieve the pipeline metrics if you have defined them in your code.

Note that the 'simple metrics' are shown in a table, but the 'lists metrics' are shown with graphs so that you can see how they change during execution. For example, here we follow the evolution of loss and accuracy over the epochs of our model training.

understand_ML_exec_3

Info

It is also possible to retrieve this information from the SDK using the functions sdk.get_metrics(name, pipeline_name, deployment_name, execution_id) and sdk.get_list_metrics(name, pipeline_name, deployment_name, execution_id), more information here.

Finally, the execution logs are also available in the associate tab. Note that the logs, like the other information, are not automatically reflected in the web interface, hence the buttons with arrows for refreshing the page.

Info

Here again, an SDK function is available with sdk.get_pipeline_execution_logs(pipeline_name, execution_id, from_datetime, to_datetime, limit).

How to compare multiple executions?
Compare execution with a table of comparison
Let's go back to the source code of the step from the beginning. This code is designed to train a deep learning model. This model has three hyper-parameters (the learning rate, the number of epochs, and the batch-size) which are associated with inputs to the step.

We can also see that in the step code, there are simple metrics and lists to track performance during and at the end of training.

Here, we'll vary the hyperparameters over several training sessions to find the best values. We'll vary the learning rate and batch size with these values:

Learning rate	Number of epochs	Batch size
0.01	10	32
0.01	10	64
0.01	10	128
0.001	10	32
0.001	10	64
0.001	10	128
0.0001	10	32
0.0001	10	64
0.0001	10	128
Each line in this table represents an execution with its hyper-parameters and therefore a training run of the model.

Instead of looking at the executions one by one in execution tracking, we go to Executions > Execution Comparison. Then select its environment, to finally see the table with all the executions.

understand_ML_exec_4

There are 3 important elements on this page:

The table, the central element of the page. In this table, each row represents an execution, except for the first row, which is the column header. Each column represents information about the executions.
These selectors can be used to add more or less information to the table, allowing inputs, metrics, etc. to be displayed or not. The more information you select, the more columns the table will have.
Another tab is available on this page for viewing the metrics lists, but we'll come back to this later.
To start with, I'm going to select the meta-data, inputs, simple metrics, and list metrics. We don't need the outputs in this case. If I want, I can even select precisely the inputs and metrics I've used in my executions.

Then, in the header of the table, I'll filter the pipeline names so that I only have the executions from the pipeline I've used.

Note

All filter settings are available from the Filters button at the top right of the screen.

Finally, I'll sort according to precision by clicking on the little arrow in the column header.

That's it, I've sorted my executions to find the parameters that give me the best accuracy for my model. In my case, the best result is obtained with a learning rate of 0.001 and a batch size of 128.

understand_ML_exec_5

We could also have sorted according to metric lists, with the difference that you have to select your calculation mode before sorting. In fact, since we're just displaying a number representing the list (with the average, the last number, the minimum, etc.), you do this by clicking on the tag at the top of the column (here with last in the screenshot below):

understand_ML_exec_6

CCompare the list metrics of several executions
If you want to see all the values in the execution lists, you can also display the list metrics in their entirety to compare them between executions. To do this, select the executions you want to view using the eye on the left of the table, then go to the visualize tab (top right).

Note

Only list metrics executions have a selectable eye.

On this screen, there is a graph for each available metrics list, and each execution is represented by a color. So you can compare the evolution of your metrics between each training session.

understand_ML_exec_7

Info

You can hide executions by clicking on their names in the legend.

How to follow a pipeline in production?
Pipeline monitoring
Once our model has been trained and selected, we're going to expose it in an endpoint so that it can be used from any application. To do this, we're already going to need source code for an inference pipeline, this is the 2áµ‰ Python code given at the beginning. Note that this code reuses the validation dataset, to do the inference, for simplicity. We can, therefore, score each prediction and put the result in a score metric:

1: The prediction is accurate
0: The prediction is false
We create the step, the pipeline, and the endpoint deployment with the associated input/output. Our model is now ready to make predictions. For each prediction, we can track executions using the tools we've already seen.

In addition, you can also monitor our executions more globally over time by going to Monitoring > Pipeline metrics. On this page, you can see the evolution of the single metrics over time for any selected deployment.

In our case, we can track, execution after execution, the prediction score (true or false) of the model:

understand_ML_exec_8

Info

You can select a date range in the top right-hand corner. You can also zoom in on a selected range in the graph.

Resource monitoring
When you have several models in production (or even in training), you may want to have information about the health of your environment's infrastructure. This helps to ensure that the size of the environment corresponds to the computation/storage requirements and also to identify any problems.

The Monitoring > Resource metrics page allows you to see any over-use of resources (with the cards at the top), as well as their evolution over time (with the graphs), once an environment has been selected.

understand_ML_exec_9

As a reminder, an environment is made up of workers, which are the calculation units within the environment. Each worker, therefore, has a dedicated curve (selectable from the legend) plus a curve for all the cumulative workers.

You can download this data in .csv format using the download button. The data downloaded will be that selected by date and worker (as for the graphs).

Info

It is also possible to retrieve data with the SDK using the sdk.get_resource_metrics(start_date, end_date, csv) function.

The csv (binary) parameter can be used to retrieve data in .csv format, like the button, or in a Python dictionary.

Conclusion
In this page, we have covered essential aspects of tracking, monitoring, and analyzing machine learning executions on the Craft AI platform. By understanding how to find and obtain details about executions, track input and output, retrieve metrics and logs, and compare multiple executions, data scientists can effectively leverage the platform's capabilities.

Additionally, we explored the process of following a pipeline in production, including pipeline monitoring and resource monitoring. These practices ensure that models are deployed and running smoothly, with the ability to assess performance and resource utilization over time.

By mastering these tools and techniques, data scientists can make informed decisions, optimize models, and achieve the full potential of the MLOps platform provided by Craft AI.

Deploy in low-latency
Summary
What is low-latency mode?
Deployment Creation Step by Step
Monitoring Low Latency
Preloading data for low-latency
What is low-latency mode?
Introduction
The Craft AI platform offers two pipeline deployment modes:

Elastic: This is the default mode. It emphasizes simplicity of use.
Low-latency: This mode is designed to achieve faster pipeline execution times.
A pipeline deployment always has a mode, and an execution rule such as "endpoint" or "periodic". More detail about deployment on this page.

Before delving into how the low-latency mode operates, let's establish some key points about the deployment modes.

Note

The new deployment method does not give rise to any additional financial costs, as it remains in the same environment.

Elastic Mode
This is the default mode for deployments. In this mode, executions are stateless.

This means that executions in Elastic mode are self-contained and independent from each other, since a unique temporary container is created for each execution. In this mode, executions use all the available computing resources automatically, and no resource is used when there is no execution in progress.

Advantages:

Automatic resource management
No memory side effects executions
Disadvantage:

Slower individual execution time
Technical information

In this context, a "container" refers to a pod in Kubernetes. This mode creates a pod with the required dependencies for each execution, executes the code, and then destroys the pod. This approach enables stateless execution but contributes to an increase in execution time.

Low-latency Mode
With this mode, an execution container called a pod is initialized ahead of time, ready for executions.

As a result, the execution time in this mode is faster than in elastic mode. It is designed for use cases that require fast response times or have long initialization times.

Note

The faster execution time in this mode comes from gains in how executions are started and stopped, it does not mean that computation is faster. The time taken to compute the same code during an execution in both modes remains the same, depends on the computation resources of the environment.

All executions for a low-latency deployment share the same pod, where memory is shared between executions.

A low-latency deployment pod uses computation resources from the environment, even without any execution in progress. And all executions for a deployment run on the same pod. So you need to manage the computation resources of the environment.

Advantage:

Faster execution time
Disadvantages:

Manual resource management
Memory side effects between executions
Technical details

When creating a low-latency deployment, the associated pod is created before any execution can start. It starts the process where executions will run, with the dependencies required to run your code. A pod can only handle one execution at a time, but Python global variables are shared between executions.

Info

The support for multiple pods per deployment or multiple executions per pod is coming soon.

Summary
For real-time response, use low-latency mode. Otherwise, keep the default mode, elastic mode.

It is important to note that selecting low-latency mode results in a shared execution context between executions in the same deployment, and in a continuously active pod, which requires monitoring resource usage throughout the deployment's lifespan.

Note

The run_pipeline() function does not create a deployment, but its behavior is similar to that of the elastic deployment mode.

schema_low-latency

Deployment Creation
In this section, we'll look at the steps involved in creating a low-latency deployment using the Craft AI SDK.

Note

If you have already initialized your SDK with your environment and are familiar with the creation and use of elastic deployment, this section is not applicable. Otherwise, please refer to the relevant documentation here.

To achieve our first low-latency deployment, we will utilise a basic Python script that multiplies two numbers:

multipli.py
def entryStepMultipli(number1, number2):
    return {"resultMulti": number1 * number2}
Warning

Remember to push this source code to Git so that the platform can access the Python script for execution.

The approach for creating the step and associated pipeline remain the same, regardless of the chosen deployment mode:

# IO creation 
step_input1 = Input(
    name="number1",
    data_type="number",
)

step_input2 = Input(
    name="number2",
    data_type="number",
)

step_output1 = Output(
    name="resultMulti",
    data_type="number",
)

# Step creation
sdk.create_step(
    function_path="src/multipli.py",
    function_name="entryStepMultipli",
    step_name="multi-number-step",
    container_config = { 
    "local_folder": "my_step_folder/",
  },

    inputs=[step_input1, step_input2],
    outputs=[step_output1],
)

# Pipeline creation 
sdk.create_pipeline(pipeline_name="multi-number-pipl", step_name="multi-number-step")
The mode parameter is initially set to low_latency upon creation.

However, it takes a few tens of seconds for the deployment to become active. You can use a loop to wait for it to be ready, as shown here.

# Deployment creation 
endpoint = sdk.create_deployment(
    execution_rule="endpoint",
    pipeline_name="multi-number-pipl",
    deployment_name="multi-number-endpt",
    mode="low_latency",
)

# Waiting loop until deployment is ready 
status = None
while status != 'success':
    status = sdk.get_deployment("multi-number-endpt")['status']
    if status != 'success':
        print("waiting endpoint ready...", sdk.get_deployment("multi-number-endpt")['status'])
        time.sleep(5)
deploi_info = sdk.get_deployment("multi-number-endpt")
print(deploi_info)
After deployment, it operates like an elastic deployment. It can be triggered through the SDK or other methods such as Postman, curl, or JavaScript requests:


Python SDK
Curl
Javascript
sdk.trigger_endpoint(
    endpoint_name=deploi_info["name"],
    endpoint_token=deploi_info["endpoint_token"],
    inputs={"number1": 3, "number2": 4},
    wait_for_results=True,
)

Tip

As with elastic deployments, low-latency deployments linked to the pipeline can be viewed on the pipeline page of the web interface, along with the relevant information and executions.

Monitoring Low Latency
As previously mentioned, deploying with low-latency introduces additional complexity. To effectively monitor deployment activity, the platform offers various information:

Status: Information on the deployment lifecycle at a given point in time.
Logs: Historical and detailed information on the deployment lifecycle
Version: Information on the deployment update
Status
Low latency deployments have two additional specific statuses:

deployment_health: Represents the potential availability of the deployment. If enabled and this status is set to Ready, then the deployment is ready to receive requests.
status: Represents the loading of a pod into the deployment. Note that this does not necessarily correlate with the health of the deployment.
Warning

These two statuses are different from the 'is_enabled' parameter, which represents the user's chosen deployment availability.

These statuses are available in the return object of the get_deployment() function:

sdk.get_deployment(
    deployment_name="multi-number-endpt"
)
Note

The pod has a specific status in addition to that of deployment.

Deployment logs
During its lifetime, a low-latency deployment generates logs that are not specific to any execution but are linked to the deployment itself. You can use the get_deployment_logs() function in the SDK to get them.

from datetime import datetime, timedelta

sdk.get_deployment_logs(
    deployment_name="multi-number-endpt",
    from_datetime=datetime.now() - timedelta(hours=2), 
    to_datetime=datetime.now(), 
    type="deployment",
    limit=None
)
Deployment update
A low-latency deployment can be used to reload the associated pod. To do this, you can call the SDK's update_deployment() function:

sdk.update_deployment(
    deployment_name="multi-number-endpt"
)
Preloading data for low-latency
Warning

Configuration on demand is an incubating feature.

Concept
When using low-latency mode, it is important to note that this implies continuity between executions. This is because the pod that encapsulates the executions remains active throughout the lifetime of the deployment.

As a result, there is memory permeability between executions. Each execution runs in a different thread in the same process. While this feature can be advantageous, it must be used with care. It allows data to be loaded into memory (RAM and VRAM) prior to an execution by using global variables in Python.

How to do that
The code, specified in the function_path property, when the step was created, is imported during the creation of a low-latency deployment. This enables the loading of variables prior to the first deployment run.

Note

A global variable can also be defined â€œonlyâ€ in the first execution by creating it only in the function.

Once the data has been loaded into a global variable, it can be read in function executions.

Note: That this does not require any changes to the creation of platform objects (step, pipeline, etc.) using the SDK.

Warning

If the pod is restarted (after a standby, for example), the loaded data is reset as when the deployment was created.

Examples
Simple example
# Import lib 
from craft_ai_sdk import CraftAiSdk
import os, time

# Code run at the low latency deployment creation 
count = 0 
loaded_data = [2, 3, 5, 7, 11, 13, 17, 19, 23, 29]

print ("Init count at : " + count)

# Function who will be run at each execution 
def my_step_source_code():
    global loaded_data, count

    count += 1 

    print (count, loaded_data)
Deployment logs and logs of the first 2 runs :

Deployment's logs : 

> Pod created successfully
> Importing step module
> Init count at : 0
> Step module imported successfully
> Execution "my-pipeline-1" started successfully
> Execution "my-pipeline-2" started successfully

"my-pipeline-1" logs :

> 1 [2, 3, 5, 7, 11, 13, 17, 19, 23, 29]

"my-pipeline-2" logs :

> 2 [2, 3, 5, 7, 11, 13, 17, 19, 23, 29]
Note

You can access to deployment logs by using the SDK function sdk.get_deployment_logs().

Example of usage:

logs_deploy = sdk.get_deployment_logs(
    deployment_name="my-deployment",
)

print('\n'.join(log["message"] for log in logs_deploy))
Example of a step with LLM preloading
import time
from vllm import LLM, SamplingParams
from craft_ai_sdk import CraftAiSdk
import os
from io import StringIO

sampling_params = SamplingParams(temperature=0.8, top_p=0.95, max_tokens=1024)
llm = LLM(
    model="TheBloke/Mistral-7B-Instruct-v0.2-AWQ",
    quantization="awq",
    dtype="half",
    max_model_len=16384,
)

def persitent_data_step(message: str):
    global llm

    output = llm.generate([message], sampling_params)[0]

    return {
        "results": output.outputs[0].text,
    }


Connect a Git repository to the platform
The basic method to retrieve the source code of a pipeline is by specifying a local file that contains the script in .py format. However, it is also possible to retrieve the script directly from a Git repository (GitHub or GitLab). Connecting the Git repository to your platform offers several advantages over using local files.

This documentation explains the benefits of this integration and the steps required to set up this connection securely and efficiently.

Why connect a Git repository?
Limitations of using local files
Typically, the platform is used solely with local files for sharing the source code of pipelines. This approach has several drawbacks:

Lack of versioning: It is difficult to track changes made to the code.
Limited sharing: Sharing code with other team members is complex and inconvenient.
Poor practice for production: Working with local files is not recommended for production environments due to the lack of controls and security measures.
Proposed solution
To overcome these limitations, we propose reading the code directly from a Git repository. This allows for easier versioning and code sharing. This practice is strongly encouraged for production projects, but not exclusively.

To achieve this:

The user must create a deployment key.
The public key must be added to the Git repository.
The private key must be shared with the platform so it can access the repository.
Let's see how to do this in detail.

How to connect Git repository ?
Providing information to the Git repository
To allow the Git repository to recognize and authorize connections from the platform, it is necessary to provide an SSH key pair:

Step 1 : Generating the deploy key
For security reasons, to get access to your Git repository, the platform uses a Deploy Key with the RSA SSH KEY standard. The deploy key is a special key that grants access to a specific repository; it is not the same as personal keys used commonly by users to access their repositories, although they are both SSH keys.

The deploy key has two elements:

The public key, which must be set in the GitHub administration settings for the repository.
The private key, which must be sent to the Craft AI MLOps Platform, so it can access the repository.
First, you will need to generate an SSH key on your computer:

On Linux and macOS
On window
Step 2 : Adding the public key to the Git repository
Now, you have to add the public key to the Git repository settings as a deployment key.

For GitHub :

Head to the homepage of your repository on GitHub.

Go to the Settings page.

Once there, select the tab on the left named Deploy Keys

Select Add deploy key on the Deploy Keys page.

deploy_key

Insert the name you want for your deploy key

Copy/paste the public key (content of *your-key-filename*.pub) in the second text box.

Click on â€œAdd keyâ€ (you donâ€™t need to allow write access)

For GitLab :

Head to the homepage of your repository on GitLab.
Click on Settings (left bar) then go to Repository
Click on Expand in the Deploy keys section
Insert the name you want for your deploy key.
Copy/paste the public key (content of *your-key-filename*.pub) in the second text box.
Click on Add key (you donâ€™t need to Grant write permissions to this key)
Providing Information to the platform
There are two methods to integrate the SSH key on the platform side:

In the step creation information
In the project information
Step 3a : During step creation
Parameters can be configured during each step creation just like with a local folder. To do this, you need to add the necessary information (repository URL, branch, private key) directly into the container configuration.

Let's take this file structure as an example:

.
â”œâ”€â”€ sdk-platform-script.py
â”œâ”€â”€ my-git-repo/
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ requirements.txt 
â”‚   â””â”€â”€ src/
â”‚       â””â”€â”€ my-source-code.py
â””â”€â”€ keys/
    â”œâ”€â”€ my-key-filename
    â””â”€â”€ my-key-filename.pub
The script sdk_platform_script.py is responsible for creating platform objects using the SDK. The file my_source_code.py contains the Python code that will be executed in the pipeline, and the keys directory contains the previously generated keys.

When creating a step, you can configure your SDK script in this way to create a step from the code in the Git repo:

sdk-platform-script.py
with open('keys/my-key-filename', 'r') as file:
    private_key_value = file.read().rstrip()


sdk.create_step(
    function_path="src/my-source-code.py",
    function_name="my-function-name", 
    step_name="my-step-name",
    container_config = {
        "repository_url": "git@github.com:my-account/my-git-repo.git",
        "repository_deploy_key": private_key_value,
        "repository_branch": "main"
    }
)
Step 3b : In project information
Alternatively, you can provide the repository information in the project settings. Once configured, if no additional information is provided during the pipeline creation, it will use the default project settings.

To do this, go to your project's settings page and enter these 3 parameters:

Repository URL : Enter the SSH URL of your repository.

How to get my repository URL
Deploy key : Enter your Github / GitLab private key.

Warning

Remember to keep the begin and end tags when you copy/paste the key. It should look like this :

-----BEGIN RSA PRIVATE KEY-----
MIIJKQIBFSKCAgEAwH/zbeYm3M7elJHIjQTiO2+2QdTOh3ebvZotNQNATJ4UIqVN
T9P2xN3Xd/27w8/jv9wmGqHzSVyEo53FfnyDm2zlFvqImRZm3znujA9bbp00itB5
...
Bo1gJMJxYJ4npi+0VULc33Ao6FzOfGxSACoTA/gG/q7LHO68c6Zgz+dI/ekDqG7C
Gx52WhCP26GdneD/EhPgcUh41FzDbgO2BBIboNnrJLQzSQboK8JNrsislPr7
-----END RSA PRIVATE KEY-----
Default branch : Enter the Git branch you want as default for this project. If this field is empty, we will use the default Git branch. It will be possible to choose a different default branch within an environment.

Once this project information is saved, you can set up your steps so that the platform will use your Git repository by default:

sdk.create_step(
    function_path="src/my-source-code.py",
    function_name="my-function-name", 
    step_name="my-step-name"
)
Note

In the case where the Git repo information has been defined in the environment settings page as well as in the step creation function, the information defined in the step creation function takes priority.


Parallelism Deployment
Explanation
Parallelism in deployment allows multiple tasks or executions to run concurrently on the same pod, improving the efficiency and speed of operations. This is particularly useful in data processing, machine learning inference, and other computationally intensive tasks where waiting for one task to complete before starting another can lead to significant delays.

Note

The platform offers two types of parallelism:

Simple Parallelism: This allows deploying Python code in parallel without any changes; however, it is not compatible with all Python libraries.
Advanced Parallelism: More robust, it is compatible with the majority of Python libraries but requires modifications to your source code.
We'll go into more detail later.

Advantages
Reduced Waiting Time: Users experience less waiting time for deployments to complete.
Improved Resource Utilization: Better usage of available computational resources, ensuring that they are not idle while waiting for other tasks to complete.
Scalability: Parallel executions enable the system to handle larger workloads by distributing tasks across multiple threads or asynchronous operations.
Warnings
Complexity: Managing parallel executions adds complexity to the deployment process, requiring careful handling of shared resources and potential concurrency issues.
Thread Safety: Only thread-safe code is guaranteed to run in parallel without errors. Non-thread-safe code may lead to race conditions and other concurrency issues.
Shared Execution Space: When using parallel executions, all tasks share the same execution space, which can lead to conflicts.
Environment Variables: Environment variables can have unexpected values if they are modified during execution by another parallel task.
Resource Sharing: Global variables, GPU memory, and disk files are shared between multiple executions, so you must be mindful of potential race conditions and concurrency issues.
Log Management: Using some libraries with async/threaded methods in your code may cause logs to be associated with the wrong running execution. Logs are associated with executions through Python contextvars.
How to Deploy with Parallelism
Example of SDK Code
To create a deployment with parallelism enabled, use the create_deployment function from the CraftAiSdk. Below are examples of how to configure this:

# Setting a specific number of parallel executions
sdk.create_deployment(
    execution_rule="endpoint",
    pipeline_name="my_pipeline_name",
    deployment_name="my_deployment_name",
    mode="low_latency",
    enable_parallel_executions=True,  # Default is False
    max_parallel_executions_per_pod=10  # Default is 6
)
Note

If the number of concurrent executions exceeds the maximum allowed (max_parallel_executions_per_pod), additional executions will be queued until a slot becomes available.

Tip

You can also create your deployment in the Web UI and enable parallelism from there (an option available in the advanced settings on the deployment creation page).

Simple Parallelism
Explanation
Simple parallelism utilizes Python's threading capabilities to run multiple threads concurrently. This allows tasks to be executed in parallel without changes to the existing codebase. Some libraries commonly used in data science do not work with this parallelism mode, so be careful to take it into account.

Advantages:

Minimal Code Changes: Requires little to no changes to existing code.
Ease of Implementation: Easy to implement and use for tasks that are I/O bound or where the GIL (Global Interpreter Lock) is not a major concern.
Disadvantages:

Limited Compatibility: Not all libraries are thread-safe and may not work correctly with threading.
GIL Restrictions: Python's GIL can limit the effectiveness of threading for CPU-bound tasks.
Hello world use case
Here is an example of how to use threading for simple parallelism:

Source code of the pipeline
code_step.py
# This script defines a function `my_fct` which prints the current time,
# waits for 5 seconds, and then prints the time again.
from datetime import datetime
import time

def my_fct():
    print(f"Hello: {datetime.now()}")
    time.sleep(5)
    print(f"World: {datetime.now()}!")
Creation of the pipeline and deployment
script_sdk_create_obj.py
# This script creates a pipeline and a deployment using the Craft AI SDK. 
# It uploads the code defined in `code_step.py` and sets up parallel executions for the deployment.
from craft_ai_sdk import CraftAiSdk

# Initialize the Craft AI SDK with the specified environment URL and token.
sdk = CraftAiSdk(environment_url="my-environment-url", sdk_token="my-sdk-token")

# Pipeline creation as usual 
sdk.create_pipeline(
    function_path="code_step.py",
    function_name="my_fct", 
    pipeline_name="my-pipeline-name",
    container_config = { 
        "local_folder": "src/",
    },
)

# Deployment creation with parallelism enabled 
sdk.create_deployment(
    execution_rule="endpoint",
    pipeline_name="my-pipeline-name",
    deployment_name="my-deployment-name",
    mode="low_latency", # Mandatory, parallelism only work on low-latency mode

    enable_parallel_executions=True,
    max_parallel_executions_per_pod=5
)
Trigger the deployment
script_call_pipeline.py
# This script makes multiple concurrent requests to a deployment on the Craft AI platform.
from craft_ai_sdk import CraftAiSdk

deployment_name = "my-deployment-name" # Define the deployment name and the number of requests to be launched.
num_requests = 20  # Number of concurrent requests to launch.

# Initialize the Craft AI SDK with the specified environment URL and token.
sdk = CraftAiSdk(environment_url="my-environment-url", sdk_token="my-sdk-token")

# Retrieve the endpoint information for the given deployment name using the SDK.
endpoint_info = sdk.get_deployment(deployment_name)

for _ in range(num_requests):
    # Launch executions simultaneously on the platform, results available on the execution tracking page 
    sdk.trigger_endpoint(deployment_name, endpoint_info["endpoint_token"], wait_for_results=False)
Note

Once launched, the executions are visible in execution tracking and will be processed simultaneously in groups of 5.

Advanced Parallelism
Explanation
Advanced parallelism leverages Python's AsyncIO library to manage asynchronous tasks. This approach is more suited for tasks that require concurrent I/O operations, such as network requests or reading from multiple sources simultaneously.

AsyncIO is a library designed to handle asynchronous operations using the async/await syntax.

The platform automatically detects and implements asynchronous parallelism based on the userâ€™s source code, so you don't need to add anything extra during deployment creation.

To trigger advanced parallelism, you just need to add the async keyword to the entry function of your step code. Then, you can use the await keyword where necessary.

Example of usage out of the platform:

import asyncio

async def greet(name):
    print(f"Hello, {name}")
    await asyncio.sleep(1)
    print(f"Goodbye, {name}")

async def main():
    await asyncio.gather(
        greet("Alice"),
        greet("Bob"),
        greet("Charlie")
    )

# Running the main function on a local device
asyncio.run(main())
>>> Hello, Alice
>>> Hello, Bob
>>> Hello, Charlie
>>> Goodbye, Alice
>>> Goodbye, Bob
>>> Goodbye, Charlie
Advantages:

Broad Library Support: Works with a wide range of libraries and frameworks.
Efficient I/O Handling: Ideal for tasks that involve I/O operations.
Disadvantages

Code Adaptation: Requires changes to the code to use async/await syntax.
Learning Curve: May have a steeper learning curve compared to threading.
Use Cases hello world
Let's take the previous example of a "hello world" and adapt it to the asyncio library.

Source code of the pipeline
code_step.py
# This script defines an asynchronous function `my_fct` which prints the current time,
# waits for 5 seconds asynchronously, and then prints the time again.
from datetime import datetime
import time
import asyncio

async def my_fct():
    print(f"Hello: {datetime.now()}")
    await asyncio.sleep(5)
    print(f"World: {datetime.now()}!")
Warning

It's the only code that needs to be changed compared to the simple parallelism example.

Three changes took place:

Addition of the import statement for the asyncio library.
Addition of the async keyword before the function definition.
Replacement of the time.sleep(5) function with asyncio.sleep(5). Be careful to include the await keyword before the function.
Creation of the pipeline and deployment
Tip

It's exactly the same code as the simple parallelism example.

script_sdk_create_obj.py
# This script creates a pipeline and a deployment using the Craft AI SDK. 
# It uploads the code defined in `code_step.py` and sets up parallel executions for the deployment.
from craft_ai_sdk import CraftAiSdk

# Initialize the Craft AI SDK with the specified environment URL and token.
sdk = CraftAiSdk(environment_url="my-environment-url", sdk_token="my-sdk-token")

# Pipeline creation as usual 
sdk.create_pipeline(
    function_path="code_step.py",
    function_name="my_fct", 
    pipeline_name="my-pipeline-name",
    container_config = { 
        "local_folder": "src/",
    },
)

# Deployment creation with parallelism enabled 
sdk.create_deployment(
    execution_rule="endpoint",
    pipeline_name="my-pipeline-name",
    deployment_name="my-deployment-name",
    mode="low_latency", # Mandatory, parallelism only work on low-latency mode

    enable_parallel_executions=True,
    max_parallel_executions_per_pod=5
)
Trigger the deployment
Tip

It's exactly the same code as the simple parallelism example.

script_call_pipeline.py
# This script makes multiple concurrent requests to a deployment on the Craft AI platform.
from craft_ai_sdk import CraftAiSdk

deployment_name = "my-deployment-name" # Define the deployment name and the number of requests to be launched.
num_requests = 20  # Number of concurrent requests to launch.

# Initialize the Craft AI SDK with the specified environment URL and token.
sdk = CraftAiSdk(environment_url="my-environment-url", sdk_token="my-sdk-token")

# Retrieve the endpoint information for the given deployment name using the SDK.
endpoint_info = sdk.get_deployment(deployment_name)

for _ in range(num_requests):
    # Launch executions simultaneously on the platform, results available on the execution tracking page 
    sdk.trigger_endpoint(deployment_name, endpoint_info["endpoint_token"], wait_for_results=False)
Use case vLLM text generation
Let's now look at an example of an LLM that generates text (including with multiple executions in parallel on the platform). For this, we use the vLLM library, which allows managing concurrent text generation on the same GPU with asynchronous executions on the platform.

Source code of the pipeline
For this use case, we need a requirements.txt file like this:

requirements.txt
vllm
Source code of the step:

code_step_vllm.py
import datetime
from vllm import SamplingParams, AsyncLLMEngine, AsyncEngineArgs

MODEL_NAME = "TheBloke/Mistral-7B-Instruct-v0.2-AWQ"
engine_args = AsyncEngineArgs(
    model=MODEL_NAME,
    quantization="awq",
    dtype="half",
    max_model_len=8000,
)
llm = AsyncLLMEngine.from_engine_args(engine_args, start_engine_loop=True)
sampling_params = SamplingParams(temperature=0.8, top_p=0.95, max_tokens=512)

id = 0

async def main(prompt):
    global id
    id += 1

    prompt_formatted = f'''<s>[INST] {prompt} [/INST]'''

    output = None
    async for out in llm.generate(prompt_formatted, sampling_params, id):
        output = out.outputs[0].text

    return {'output': output}
Creation of the pipeline and deployment
The creation of objects is almost the same as before, with the difference being that we add a text input and output.

script_sdk_create_obj.py
# This script creates a pipeline and a deployment using the Craft AI SDK. 
# It uploads the code defined in `code_step_vllm.py` and sets up parallel executions for the deployment.
from craft_ai_sdk import CraftAiSdk
from craft_ai_sdk.io import Input, Output

# Initialize the Craft AI SDK with the specified environment URL and token.
sdk = CraftAiSdk(environment_url="my-environment-url", sdk_token="my-sdk-token")

# Pipeline creation as usual 
sdk.create_pipeline(
    function_path="code_step_vllm.py",
    function_name="main", 
    pipeline_name="my-pipeline-name",
    container_config = { 
        "local_folder": "src/",
        "requirements_path": "requirements.txt"
    },
    inputs=[Input(name="prompt", data_type="string")],
    outputs=[Output(name="response", data_type="string")],
)

# Deployment creation with parallelism enabled 
sdk.create_deployment(
    execution_rule="endpoint",
    pipeline_name="my-pipeline-name",
    deployment_name="my-deployment-name",
    mode="low_latency", # Mandatory, parallelism only work on low-latency mode

    enable_parallel_executions=True,
    max_parallel_executions_per_pod=5
)
Trigger the deployment
script_call_pipeline.py
# This script makes multiple concurrent requests to a deployment on the Craft AI platform.
from craft_ai_sdk import CraftAiSdk

deployment_name = "my-deployment-name" # Define the deployment name and the number of requests to be launched.
num_requests = 20  # Number of concurrent requests to launch.

# Initialize the Craft AI SDK with the specified environment URL and token.
sdk = CraftAiSdk(environment_url="my-environment-url", sdk_token="my-sdk-token")

# Retrieve the endpoint information for the given deployment name using the SDK.
endpoint_info = sdk.get_deployment(deployment_name)

for _ in range(num_requests):
    # Launch executions simultaneously on the platform, results available on the execution tracking page 
    sdk.trigger_endpoint(deployment_name, endpoint_info["endpoint_token"], inputs={"prompt": "Tell me a story"}, wait_for_results=False)
Note

Once launched, the execution results are visible in execution tracking.


Work with environment variables and resource monitoring
An environment is the infrastructure used by the platform to store data and run computation. Once created, you can start working in the environments by creating pipelines and running them in the platform.

In addition, you can create and save environment variables that will allow you to parameterize certain variables in order to call them when running or deploying a pipeline.

Function name	Method	Return type	Description
create_or_update_environment_variable	CraftAiSdk.create_or_update_environment_variable (environment_variable_name, environment_variable_value)	dict	To create or update an environment variable available for all pipelines executions.
list_environment_variables	CraftAiSdk.list_environment_variables()	List of dict	Get a list of all environments variables in the current environment.
delete_environment_variable	CraftAiSdk.delete_environment_variable(environment_variable_name)	dict	Delete a specified environment variable.
Set up an environment variable
An environment variable is a value that can be passed to your step code. It is used to store information that may be needed by the operating system or by applications that run on the platform (for example, by endpoints you deploy on the platform).

Tip

On this page, we will detail the usage of the SDK, but environment variables can be modified from a page in the web interface by going to: Environments > click on the three dots of the desired environment > Settings > Environment variables.

Create and update an environment variable
To create or update an environment variable available for all pipelines executions.

CraftAiSdk.create_or_update_environment_variable(environment_variable_name,
environment_variable_value)
Parameters
environment_variable_name (str) â€“ Name of the environment variable to create.
environment_variable_value (str) â€“ Value of the environment variable to create.
Returns
A dict object containing the ID of environment variable (with keys â€œidâ€)

Get the list of environment variables
Get the list of all environment variables in the current environment.

CraftAiSdk.list_environment_variables()
Parameter
No parameter

Returns
List of dicts of environment variables (with keys â€œnameâ€ and â€œvalueâ€)

Delete an environment variable
Delete a specified environment variable.

CraftAiSdk.delete_environment_variable(environment_variable_name)
Parameter
environment_variable_name (str) â€“ Name of the environment variable to delete.
Returns
Dict (with keys â€œnameâ€ and â€œvalueâ€) of the deleted environment variable

Get resource metrics
Get resource metrics of the environment into dict format (by default) or in a .csv file. You can view these metrics in graph format using the Resource Metrics web page.

CraftAiSdk.get_resource_metrics(start_date, end_date, csv=False)
Parameter
start_date (datetime.datetime) - The beginning of the period.
end_date (datetime.datetime) - The end of the period.
csv (bool) - If True, it will return a csv file as bytes.
Returns
If csv is True, it will return bytes. Otherwise, dict with:

The resource metrics, with the following keys:

additional_data (dict): Additional data with the following keys:
total_disk (int): Total disk size in bytes.
total_ram (int): Total RAM size in bytes.
total_vram (int): Total VRAM size in bytes if there is a GPU.
cpu_usage (list of dict): The CPU usage in percent.
disk_usage (list of dict): The disk usage in percent.
ram_usage (list of dict): The RAM usage in percent.
vram_usage (list of dict): The VRAM usage in percent if there is a GPU.
gpu_usage (list of dict): The GPU usage in percent if there is a GPU.
network_input_usage (list of dict): The network input usage in bytes.
network_output_usage (list of dict): The network output usage in bytes.
Each element of the lists is a dict with the following keys:

metric (dict): Dictionary with the following key:
worker (str): The worker name.
values (list of list): The values of the metrics in the following format: [[timestamp, value], ...].
Example with dict object
from datetime import datetime, timedelta

# Get the current time
current_time = datetime.now()

# Calculate the beginning of the period (last 2 hours)
begin_date = current_time - timedelta(minutes=2)

# Set end_date to the current time
end_date = current_time

# Print the results
print("begin_date:", begin_date)
print("end_date:", end_date)

res = sdk.get_resource_metrics(begin_date, end_date)

print (res)
Example of return object:

{'metrics': {'cpu_usage': [{'metric': {'worker': 'Worker-58d65a25'},
    'values': [[1707132918930, 0.00464583333348214],
     [1707132948930, 0.004708333333450027],
     [1707132978930, 0.004763888888882371],
     [1707133008930, 0.0047222222219369045],
     [1707133038930, 0.004722222221995642]]},
   {'metric': {'worker': 'Worker-6e32691a'},
    'values': [[1707132918930, 0.007868055555652481],
     [1707132948930, 0.007638888888662494],
     [1707132978930, 0.00754861111112385],
     [1707133008930, 0.0076597222224098135],
     [1707133038930, 0.007930555555419681]]},

    ...

     [1707132948930, 40546.24444444444],
     [1707132978930, 48481.6111111111],
     [1707133008930, 40639.65555555555],
     [1707133038930, 48189.03333333333]]},
   {'metric': {'worker': 'All cumulate'},
    'values': [[1707132918930, 63759.26666666666],
     [1707132948930, 56443.922222222216],
     [1707132978930, 64879.22222222221],
     [1707133008930, 46783.055555555555],
     [1707133038930, 53954.188888888886]]}]},
 'additional_data': {'total_ram': 16616214528, 'total_disk': 21462233088}}
Example with CSV file
from datetime import datetime, timedelta

# Get the current time
current_time = datetime.now()

# Calculate the beginning of the period (last 2 hours)
begin_date = current_time - timedelta(minutes=2)

# Set end_date to the current time
end_date = current_time

# Print the results
print("begin_date:", begin_date)
print("end_date:", end_date)

res = sdk.get_resource_metrics(begin_date, end_date,csv=True)

result_csv = res.decode('utf-8')

# Save the result to a CSV file
csv_filename = "resource_metrics.csv"
with open(csv_filename, "w") as csv_file:
    csv_file.write(result_csv)


Save my data on the store
Each environment has its own storage to save and retrieve any amount of data at any time, from anywhere on the web. This storage is an Amazon S3 bucket that you can manage with the Python SDK of the platform.

You can upload any kind of raw data on the store. Furthermore, the results of the pipelines and metrics generated by the platform can also be saved on the data store.

Info: For the moment, it is not possible to copy data from an environment to another. You have to download it and to re-upload it. In future versions, you will be able to transfer your data.

Info

The graphical interface of the data store will arrive later on the platform.

Summary
Get files
Get file info
Upload a file
Download a file
Delete a file
Function name	Method	Return type	Description
list_data_store_objects	CraftAiSdk.list_data_store_objects()	list of dict	Get the list of the objects stored in the data store.
upload_data_store_object	CraftAiSdk.upload_data_store_object(filepath_or_buffer, object_path_in_datastore)	None	Upload a file as an object into the data store.
download_data_store_object	CraftAiSdk.download_data_store_object(object_path_in_datastore, filepath_or_buffer)	None	Download an object in the data store and save it into a file.
delete_data_store_object	CraftAiSdk.delete_data_store_object(object_path_in_datastore)	dict	Delete an object on the data store.
Get files
Function definition
Get the list of the objects stored in the data store.

CraftAiSdk.list_data_store_objects()
Parameter
No parameter

Returns
List of objects (dict) in the data store, each object being represented as dict with :

"path" (str): Location of the object in the data store.
"last_modified" (str): The creation date or last modification date in ISO format.
"size" (str): The size of the object with units of digital storage measurement (MB, GB, ...).
Get file info
Function definition
Get information about a single object in the data store.

CraftAiSdk.get_data_store_object_information(object_path_in_datastore)
Parameter
object_path_in_datastore (str) â€“ Location of the object in the data store.
Returns
Object information, with the following keys:

"path" (str): Location of the object in the data store.
"last_modified" (str): The creation date or last modification date in ISO format.
"size" (str): The size of the object with units of digital storage measurement (MB, GB, ...).
Upload a file
Function definition
Upload a file as an object into the data store.

CraftAiSdk.upload_data_store_object(filepath_or_buffer, object_path_in_datastore)
Parameters
filepath_or_buffer (str, or file-like object) â€“ String, path to the file to be uploaded ; or file-like object implementing a read() method (e.g. via buildin open function). The file object must be opened in binary mode, not text mode.
object_path_in_datastore (str) â€“ Destination of the uploaded file.
Returns
This function returns None.

Download a file
Function definition
Download an object in the data store and save it into a file.

CraftAiSdk.download_data_store_object(object_path_in_datastore, filepath_or_buffer)
Parameters
object_path_in_datastore (str) â€“ Location of the object to download from the data store.
filepath_or_buffer (str or file-like object) â€“ String, file path to save the file to ; or a file-like object implementing a write() method, (e.g. via builtin open function). The file object must be opened in binary mode, not text mode.
Returns
This function return an None.

Delete a file
Function definition
Delete an object on the data store.

CraftAiSdk.delete_data_store_object(object_path_in_datastore)
Parameters
object_path_in_datastore (str) â€“ Location of the object to delete in the data store.
Returns
Deleted object represented as dict (with key â€œpathâ€).

Step & Pipeline
A pipeline is a machine learning workflow, consisting of one or more steps, to deploy containerised code. Like a regular function, a step is defined by the input it ingests, the code it runs, and the output it returns. You can then create a full pipeline formed with a computed acyclic graph (DAG) by specifying the output of one step as the input of another step.

introductionStepPipeline_1

ðŸ’¡ The pipelines are written in Python with SDK calls for an easy authoring experience and executed on Kubernetes for scalability. The pipelinesâ€™ functioning are based on the open-source library Argo. Each step deploy is containerised with Docker.

The main objectives of the steps and pipelines are :

Orchestrating end-to-end ML workflows
Increasing reusability of Data Science components from a project to another
Collaboratively managing, tracking and viewing pipeline definitions.
Deploying in production in few clicks with several methods (endpoint, CRON, manual, â€¦)
Enabling large scale production of Python code without refactoring to a more production friendly language
Ensuring an efficient use of compute resources, thanks to Kubernetes
Example :

ðŸ–Šï¸ Training pipeline

Data preparation and preprocessing: In this stage, raw data is collected, cleaned, and transformed into a format that is suitable for training a machine learning model. This may involve tasks such as filtering out missing or invalid data points, normalizing numerical values, and encoding categorical variables.
Model training: In this stage, a machine learning model is trained on a prepared dataset. This may involve selecting a model type, tuning hyperparameters, and training the model using an optimization algorithm.
Model evaluation: Once the model has been trained, it is important to evaluate its performance to determine how well it generalizes to new data. This may involve tasks such as splitting the dataset into a training set and a test set, evaluating the modelâ€™s performance on the test set, and comparing the results to a baseline model.
ðŸ–Šï¸ Inference pipeline

Model loading: In this stage, a trained machine learning model is loaded from storage and prepared for use. This may involve tasks such as loading the modelâ€™s weights and any associated dependencies.
Data preparation: In this stage, incoming data is cleaned and transformed into a format that is suitable for the model. This may involve tasks such as normalizing numerical values and encoding categorical variables.
Inference: In this final stage, the model is used to make predictions on the prepared data. This may involve tasks such as passing the data through the model and processing the output to generate a final prediction.


Create a step
A step is an atomic component defined by its input and output parameters and by the processing it applies. Steps are the building blocks of pipelines. In practice, a step is a function with inputs and outputs coded in Python. They are assembled to create a complete ML pipeline. The Python code (currently the only available language) used by the step is stored in a folder on your local machine or in a Git repository.

An input of a step is an object you can use inside the code. An output of a step is defined from the results of the step function. You will be able to connect inputs & outputs of a step with another step to compose a complete ML pipeline by using a directed acyclic graph (DAG).

Each step is considered as a specific container that is executed on Kubernetes.

The steps are stored in a specific environment, and only people with access to this environment can read and write the steps. By default, each step uses the values defined in the project settings. However, these values can be overridden in the step creation parameters, as detailed below.

createStep_1

Summary
Prepare your code
Define step inputs and outputs
Create a step
Function name	Method	Return type	Description
Input	Input(name, data_type="string", description="", is_required=False, default_value=None)	Input SDK Object	Create an Input object to give at create_steps() function for step a step input.
Output	Output(name, data_type="string", description="")	Output SDK Object	Create an Output object to give at create_steps() function for step a step output.
create_step	create_step(step_name, function_path, function_name, description=None, timeout_s=180, container_config=None, inputs=None, outputs=None)	list of dict[str, str]	Create pipeline steps from a source code located on a remote repository.
Prepare your code
[Optionnal] Use a Git repository
Setup Project & Environment
Currently you can create a step using the Python SDK, but not using the GUI. However, once you have created the step and associated pipeline, you will be able to see the pipeline on the UI platform.

If itâ€™s not already done, put the code of the step into a single folder, which will be sent to the platform. The file with the entry function of your step can be anywhere in your folder.

Example file tree:

.
â”œâ”€â”€ requirements.txt
â””â”€â”€ src/
    â””â”€â”€ my_entry_function_step.py
...
Example my_entry_function_step.py:

import numpy as np
# and other import

def entryStep(dataX_input, dataY_input) :

    # Some machine learning code

    return result_output
If you prefer, you can also use a GitHub repository instead of a local folder. More information can be found here.

Define step inputs and outputs
A step may need to receive some information or give some result (just like a function). To do that, we use Input and Output object. These objects allow defining the properties of the input or output that will be expected in the step. The input and output objects thus created must be given as a parameter of the step creation. Each input is defined as an Input object and, each Output is defined as an Output object, through a class available in the SDK.

Input object definition
from craft_ai_sdk.io import Input

Input(
   name="*your_input_name*",
   data_type="*your_io_data_type*",
   description="",
   is_required=True
   default_value="*default_value*"
)
Parameters
name just a name for identifying the input later.

data_type, one of the following possible types:

file: reference to binary data, equivalent to a fileâ€™s content. If the input/output is not available, an empty stream.

json: JSON-serializable Python object. The following sub-types are provided for more precise type checking, but they are all JSON

string

number

array of JSON

boolean

If the input/output is not available, None in Python

default_value (optional) - If the parameter is empty, this value will be set by default. If a deployment receives an empty parameter and already put a default value in the input, the default value of deployment will be keep.

is_required (optional, True by default) - Push an error is the input is empty.

description (optional) - This parameter precise what itâ€™s expected in this input. Itâ€™s not read by the machine, itâ€™s like a comment.

Return
No return

Output object definition
from craft_ai_sdk.io import Output

Output(
   name="*your_input_name*",
   data_type="*your_io_data_type*",
   description="",
)
Parameters
name just a name for identifying the input later.

data_type, one of the following possible types:

file: reference to binary data, equivalent to a fileâ€™s content. If the input/output is not available, an empty stream.

json: JSON-serializable Python object. The following sub-types are provided for more precise type checking, but they are all JSON
string
number
array of JSON
boolean
If the input/output is not available, None in Python

description (optional) - This parameter precise what itâ€™s expected in this input. Itâ€™s not read by the machine, itâ€™s like a comment.
Return
No return

Note

You can use craft_ai_sdk.INPUT_OUTPUT_TYPES to get all possible types in Input and Output objects.

List of all possible types :

ARRAY = "array"
BOOLEAN = "boolean"
FILE = "file"
JSON = "json"
NUMBER = "number"
STRING = "string"
Example :

from craft_ai_sdk.io import Input, INPUT_OUTPUT_TYPES

Input(
   name="inputName",
   data_type=INPUT_OUTPUT_TYPES.JSON,
)
Example for input and output
Input(
    name="inputName",
    data_type="string",
    description="A parameter for step input",
    is_required=True,
    default_value="default_content_here"
)

Output(
    name="inputName",
    data_type="string",
    description="A parameter for step input",
)
Warning

The size of the I/O must not exceed 0.06MB (except for file type).

Create a step
Function definition
Create pipeline steps from a source code located on a local folder or a Git repository.

sdk.create_step(
    function_path="src/my_reusable_funtion.py",
    function_name="my_function",
    inputs=[Input(...)],
    outputs=[Output(...)],
    name="step-name", # by default its the function name
    description="text desciption",
    timeout_s=180,
    container_config = {
        language="python:3.8-slim",
        repository_url="your-git-url",
        repository_branch="*your-git-branch* or *your-git-tag*",
        repository_deploy_key="your-private_key",
        requirements_path="your-path-to-requirements.txt",
        included_folders=["your-list-of-path-to-sources"],
        system_dependencies=["package_1", "package_2"],
        dockerfile_path="dockerfile",
        local_folder="*my-local-folder-path*"
    },
)
Parameters
function_path (str) â€“ Path to access to the file who had the entry function of the step.

function_name (str) â€“ Function name of entry function step.

inputs (list<Input>) â€“ List of step inputs.

outputs (list<Output>) â€“ List of step outputs.

name (str) â€“ Step name. By default, itâ€™s the function name. The name must be unique inside an environment and without special character ( - _ & / ? â€¦)

description (str, optional) â€“ Description of the step, itâ€™s no use by the code, itâ€™s only for user.

timeout_s (int, optional) â€“ Maximum time to wait for the step to be created. 3min by default, and must be at least 2min.

container_config (dict, optional) â€“ Dict Python object where each key can override default parameter values for this step defined at project level.

language (str, optional) â€“ Language and version used for the step. Defaults to falling back on project information. The accepted formats are python:3.X-slim, where 3.X is a supported version of Python, and python-cuda:3.X-Y.Z for GPU environments, where Y.Z is a supported version of CUDA. The list of supported versions is available here.

repository_url (str, optional) â€“ Remote repository URL.
repository_branch (str, optional) â€“ Branch name for Git repository. Defaults to None.
repository_deploy_key (str, optional) â€“ Private SSH key related to the repository.
requirements_path (str, optional) â€“ Path to the file requirement for Python dependency.
included_folders (list, optional) â€“ List of folders that need to be accessible from step code.
system_dependencies (list, optional) â€“ List of APT Linux packages to install.
dockerfile_path (str, optional) â€“ Path to a docker-file for having a custom config in step. (see the part after for more detail)
local_folder (str, optional): Path to local folder where the step files are stored, if not on a Git repository.
Note

The repository_branch parameters as well as the container_config elements (except dockerfile_path) can take one of the STEP_PARAMETER object's values in addition to theirs.

In fact, STEP_PARAMETER allows us to specify at the step level whether we want to take the project's values (default behavior) or define a null value:

STEP_PARAMETER.FALLBACK_PROJECT : Allows to take the value defined in the project parameters (default behavior if the field is not defined).
STEP_PARAMETER.NULL : Allows to set the field to null value and not to take the value defined in the project.
Example with a code step that does not need a requirement.txt and does not take the one defined in the project settings:

from craft_ai_sdk import STEP_PARAMETER

# Code for init SDK here ...

sdk.create_step(
  function_path="src/helloWorld.py",
  function_name="helloWorld",
  step_name="my_step_name",
  container_config = {
      "requirements_path": STEP_PARAMETER.NULL,
   }
)
Warning

The size of the embedded code from your folder / Git repository must not exceed 5MB. You can select the part of your folder / Git repository to import using the included_folders parameter.

If the data you want to import is larger than 5MB, you can use the data store to store it and then import it into your step.

Returns
The return type is a dict with the following keys :

parameters (dict): Information used to create the step with the following keys:
step_name (str): Name of the step.
function_path (str): Path to the file that contains the function.
function_name (str): Name of the function in that file.
description (str): Description.
inputs (list of dict): List of inputs represented as a dict with the following keys:
name (str): Input name.
data_type (str): Input data type.
is_required (bool): Whether the input is required.
default_value (str): Input default value.
outputs (list of dict): List of outputs represented as a dict with the following keys:
name (str): Output name.
data_type (str): Output data type.
description (str): Output description.
container_config (dict[str, str]): Some step configuration, with the following optional keys:
language (str): Language and version used for the step. Defaults to falling back on project information. The accepted formats are python:3.X-slim, where 3.X is a supported version of Python, and python-cuda:3.X-Y.Z for GPU environments, where Y.Z is a supported version of CUDA. The list of supported versions is available here.
repository_url (str): Remote repository url.
repository_branch (str): Branch name.
included_folders (list[str]): List of folders and files in the repository required for the step execution.
system_dependencies (list[str]): List of system dependencies.
dockerfile_path (str): Path to the Dockerfile.
requirements_path (str): Path to the requirements.txt file.
creation_info (dict): Information about the step creation:
created_at (str): The creation date in ISO format.
updated_at (str): The last update date in ISO format.
commit_id (str): The commit id on which the step was built.
status (str): The step status, if the step creation process is under 2m40s (most of the time it is), is always Ready when this function returns.
origin (str): The origin of the step, can be git_repository or local.
Note

Once our step is created, we need to create the pipeline that wraps the step. It is mandatory to create a pipeline once the step is created to be able to use it later. This technical choice was made in anticipation of future multistep functionality. This forces the use of a pipeline to contain the steps.

Liste of language available
When using a CPU environment, the language parameter must be :

python:3.8-slim.
python:3.9-slim
python:3.10-slim
When using a GPU environment, the language parameter must be :

For cuda v11.8
python-cuda:3.8-11.8
python-cuda:3.9-11.8
python-cuda:3.10-11.8

For cuda v12.1

python-cuda:3.9-12.1
python-cuda:3.10-12.1
You can also use the CPU image in a GPU environment if you don't need access to the GPU.

Example: Create step from scratch
Function usage

from craft_ai_sdk import Input, Output

input1 = Input(
    name="input1",
    data_type="string",
    description="A parameter named input1, its type is a string",
    is_required=True,
)

input2 = Input(
    name="input2",
    data_type="file",
    description="A parameter named input2, its type is a file"
)

input3 = Input(
    name="input3",
    data_type="number",
)

prediction_output = Output(
    name="prediction",
    data_type="file",
    default_value="default,content,here",
)

step = sdk.create_step(
    function_path="src/my_reusable_funtion.py",
    function_name="my_function",
    description="Apply the model to the sea",
    container_config = { 
        "local_folder": "my/path/",
    },
    inputs_list=[input1, input2, input3],
    outputs_list=[prediction_output],
        ## ...
)
Note

If you need to create a step with a more specific configuration, you can do this with a custom Dockerfile. More detail about that here.

Download step local code
Function definition
Download a step local folder as a .tgz file. Only available if step origin is local_folder.

CraftAiSdk.download_step_local_folder(
    step_name="my_step_name", 
    folder="my/path/"
)
Parameters
step_name (str) â€“ Name of the step to be downloaded.
folder (str) â€“ Path to the folder where the file will be saved.
Returns
None

Manage a step
Summary:

Find and get information about steps
Delete steps
Function Name	Method	Return Type	Description
get_step	get_step (step_name)	dict	Get information about a step.
list_steps	list_steps()	list of dict	Get a list of available steps.
delete_step	delete_step (step_name)	dict[str, str]	Delete one step.
â“ For step update and deletion, you need the name (the name you provide when creating the step) of the step you want to update/delete. You can find it with function list_steps() (see the previous part).

Find and get information about steps
To get information about a step, we need its name in the environment. You can search its name in the list of stepâ€™s name of the environment.

Get list of steps
Function definition
To get all steps available in the current environment, you can get a list of step name with this function:

CraftAiSdk.list_steps()
Returns
List of steps represented as dict with the following keys:

name (str): Name of the step.
status (str): either Pending or Ready.
created_at (str): The creation date in ISO format.
updated_at (str): The last update date in ISO format.
repository_branch (str): The branch of the repository where the step was built.
repository_url (str): The url of the repository where the step was built.
commit_id (str): The commit id on which the step was built.
origin (str): The origin of the step, can be git_repository or local.
Get information about one step
Function definition
Get all information (repository, dependency, â€¦) about one step in the current environment with its name.

CraftAiSdk.get_step(step_name)
Parameters
step_name (str) â€“ The name of the step to get.
Returns
dict: None if the step does not exist; otherwise the step information, with the following keys:

parameters (dict): Information used to create the step with the following keys:
step_name (str): Name of the step.
function_path (str): Path to the file that contains the function.
function_name (str): Name of the function in that file.
description (str): Description.
inputs (list of dict): List of inputs represented as a dict with the following keys:
name (str): Input name.
data_type (str): Input data type.
is_required (bool): Whether the input is required.
default_value (str): Input default value.
outputs (list of dict): List of outputs represented as a dict with the following keys:
name (str): Output name.
data_type (str): Output data type.
description (str): Output description.
container_config (dict[str, str]): Some step configuration, with the following optional keys:
language (str): Language and version used for the step.
repository_url (str): Remote repository url.
repository_branch (str): Branch name.
included_folders (list[str]): List of folders and files in the repository required for the step execution.
system_dependencies (list[str]): List of system dependencies.
dockerfile_path (str): Path to the Dockerfile.
requirements_path (str): Path to the requirements.txt file.
creation_info (dict): Information about the step creation:
created_at (str): The creation date in ISO format.
updated_at (str): The last update date in ISO format.
commit_id (str): The commit id on which the step was built.
status (str): Either "Pending" or "Ready".
origin (str): The origin of the step, can be git_repository or local.
Delete steps
Delete steps function
Function definition
Delete step in the environment with his name.

CraftAiSdk.delete_step(step_name, force_dependents_deletion=False)
Parameters
step_name (str) â€“ Name of the step to delete as defined in the create step function.
force_dependents_deletion (bool, optional) â€“ if True the associated stepâ€™s dependencies will be deleted too (pipeline, pipeline executions, deployments). Defaults to False.
Returns
Deleted step represented as dict (with key â€œnameâ€). The return type is a dict [str, str].

Warning

You can't delete a step that is used in a pipeline. You must delete the pipeline before or use the force_dependents_deletion parameter during step deletion.


Compose a pipeline
A pipeline is a machine learning workflow, consisting of one or more steps, to deploy code using Docker containers. By specifying the output of one step as the input of another step, the user can create a full pipeline formed with a computed acyclic graph (DAG).

Info

For the moment, the pipelines are only single-step. We are actively working on the integration of multi-steps for the next versions of the platform.

To deploy ML code in production, you need to go through a pipeline. So you have to go through a single-step pipeline in any case to perform an endpoint or another type of deployment.

Summary:

Compose a mono-step pipeline
Delete a pipeline
Find and get pipeline information
Function name	Method	Return type	Description
create_pipeline	create_pipeline(pipeline_name, step_name)	dict[str, str]	Create a pipeline containing a single step.
get_pipeline	get_pipeline(pipeline_name)	dict	Get a single pipeline if it exists.
delete_pipeline	delete_pipeline(pipeline_name, force_deployments_deletion=False)	dict	Delete a pipeline identified by its name and ID.
list_pipelines	list_pipelines()	list of dict	Get the list of all pipelines.
Create a mono-step pipeline
Before creating a pipeline, the step creation must be finished. You can check this by checking that the step's status are equal to Ready using the get_step() function.

SDK function pipeline
Function definition
pipeline = sdk.create_pipeline(
   pipeline_name="my_pipeline",
   step_name="my_step",
)
Parameters
pipeline_name (str) â€“ Name of the pipeline to create.

step_name (str) â€“ Name of the step to include in the pipeline.

!!! note The step should have the status â€œReadyâ€ before being used to create the pipeline.

Returns
Created pipeline represented as dict using this keys:

pipeline_name (str): Pipeline name.
created_at (str): Pipeline date of creation.
steps (list[str]): List of step names.
open_inputs (list[dist]): List of all input of step.
input_name (str): Name of the open input.
step_name (str): Name of the step that provides the open input.
data_type (str): Data type of the open input.
description (str): Description of the open input.
default_value (str): Default value of the open input.
is_required (bool): Whether the open input is required or not.
open_outputs (list[dist]): List of all output of step.
output_name (str): Name of the open output.
step_name (str): Name of the step that provides the open output.
data_type (str): Data type of the open output.
description (str): Description of the open output.
Information about pipeline store
Pipeline can have multiple inputs and outputs, or no one. In fact, itâ€™s dependent on the inputs and outputs of step inside. All input and output will be the same as the step inside. So, you have nothing to configure on pipeline creation for input and output.

Get information pipeline
Get information about one pipeline
Function definition
Get all information about one pipeline, referred by its name.

CraftAiSdk.get_pipeline(pipeline_name)
Parameters
pipeline_name (str) â€“ Name of the pipeline to get.
Returns
The pipeline information in a dict (or None if the pipeline does not exist), with the following keys:

pipeline_name (str): Pipeline name.
created_at (str): Pipeline date of creation.
created_by (str): ID of the user who created the deployment.
last_execution_id (str): ID of the last execution of the pipeline.
steps (list[str]): List of step names.
open_inputs (list[dist]): List of all input of step.
input_name (str): Name of the open input.
step_name (str): Name of the step that provides the open input.
data_type (str): Data type of the open input.
description (str): Description of the open input.
default_value (str): Default value of the open input.
is_required (bool): Whether the open input is required or not.
open_outputs (list[dist]): List of all output of step.
output_name (str): Name of the open output.
step_name (str): Name of the step that provides the open output.
data_type (str): Data type of the open output.
description (str): Description of the open output.
Get all pipelines
Function definition
Get the list of all pipelines on your environment.

CraftAiSdk.list_pipelines()
Returns
List of pipelines represented as dict with keys :

"pipeline_name" (str): Name of pipeline
"created_at" (str): Create date of the pipeline.
"status" (str): Status of the pipeline.
Delete a pipeline
Function definition
Delete a pipeline from the current environment.

CraftAiSdk.delete_pipeline(pipeline_name, force_deployments_deletion=False)
Parameters
pipeline_name (str) â€“ Name of the pipeline.
force_deployments_deletion (bool, optional) â€“ if True, the associated endpoints will be deleted too. Defaults to False.
Returns
The deleted pipeline and its associated deleted deployments represented as a dict with the following keys:

pipeline (dict): Deleted pipeline represented as dict with the following keys:
name (str): Name of the deleted pipeline.
deployments (list): List of deleted deployments represented as dict with the following keys:
name (str): Name of the deleted deployments.
execution_rule (str): Execution rule of the deleted deployments.
Warning

You can't delete a pipeline that is used in a Deployment

Run a pipeline
Now that your pipeline is created you can run it directly from the SDK. Or you can configure a deployment.


Deployment
A deployment is a way to trigger an execution of a Machine Learning pipeline in a repeatable and automated way. Each pipeline can be associated with multiple deployments.

For each deployment, you can use 2 distinct execution rules :

by endpoint (web API)
by periodic trigger (CRON)
In addition, for each deployment, you will need to connect the pipeline inputs and outputs with the desired sources and destinations. When one of the deployment conditions is met, the pipeline is executed by using the computing resources available in its environment.

The results of the execution (predictions, metrics, data, ...) can be stored in the data store of the environment and can be easily retrieved by the users. You can find all the information about the executions in the execution tracking section.



In addition to the deployment functionality, it is possible to run a pipeline directly without deploying it. This allows you to run your pipeline on the fly without having to create a specific deployment, which is very useful during the experimentation phase.

The main objectives of the deployments are :

No longer take 6 months to deploy ML models in production but a few clicks!
Automating the execution of the pipelines to save time for Data Science teams
Creating secured web API to deliver pipeline results to external users without any DevOps skills
Automatically triggering re-training pipelines when model performance drops
Visualizing pipeline executions, experiment tracking, and ML artifacts


Define the pipeline sources and destinations
A deployment is a way to run a Machine Learning pipeline in a repeatable and automated way. First, you have to choose one of the 2 deployments methods.

Then, you need to connect the pipeline inputs and outputs with the desired sources and destinations.

Sources : This is the origin of the data that you want to connect to the pipeline inputs. The data can come from the data store, from environment variables, from constants or from the endpoint (if this deployment method has been chosen).

Destinations : This is the data drop point that you want to connect to the pipeline outputs. The data can go to the data store or to the endpoint (if this deployment method has been chosen).



Note

On this page, we will mainly focus on the platform's SDK interface.

However, it is possible to deploy a pipeline directly from the web interface by going to the Pipelines page, selecting a pipeline and then clicking the Deploy button.

Summary
General function of I/O mapping
Create input mapping
Create output mapping
Objects name	Constructor	Return type	Description
InputSource	InputSource(step_input_name, required=False, default=None)	InputSource Object	Create a mapping source object for deployment.
OutputDestination	OutputDestination(step_output_name, endpoint_output_name, required=False, default=None)	OutputDestination Object	Create a mapping destination object for deployment.
General function of the I/O mapping
Mapping rules
When you start a new deployment, the data flow is configured with a mapping. You can create this mapping in two ways: auto mapping (only available with endpoint trigger) or manual mapping in the SDK or UI.

Auto mapping automatically maps all inputs to endpoint variables for sources. If you need a different mapping or another trigger, you must map your inputs and outputs manually.

Auto mapping Example



For each input / output, you had defined a data type in step. This data type will be the same as the mapped step input / output. You need to map the good source and destination with the good data type :

Endpoint (input and output) â†’ Variable (string, number, ...) and file
Data store â†’ Only file
Constant â†’ Only variable (string, number, ...)
Environment variable â†’ Only variable (string, number, ...)
None â†’ Variable (string, number, ...) and file
Limitations
The inputs (as outputs) of a step mapped through the endpoint (default mapping) can consist of only a single file or multiple variables. Thus, if you use deployment by triggering an endpoint, you cannot have multiple files as inputs or outputs due to a technical limitation of the API calls.

To have multiple files as the inputs or outputs of an endpoint, you can compress the files into a single file (for example, with the [tar]{.title-ref} command) to be sent in the API call.

Examples: You can do


Warning

The data store is not yet available for the input/output mapping, but it's coming soon, so stay tuned.





Examples: You can't do






Creation input mapping
Import Dependencies
Before creating a mapping between the input/output of a pipeline and the sources/destinations of an endpoint, you need to import the InputSource and OutputDestination objects from the SDK.

from craft_ai_sdk.io import InputSource, OutputDestination
Function Definition
For each input of your pipeline, in manual mapping, you need to create a mapping object that will be given to the create_deployment() function.

boat_endpoint_input = InputSource(
    step_input_name="apply_model",

    ## Choose from one of these parameters
    endpoint_input_name="boat", ## For mapping with an endpoint
    environment_variable_name=None, ## For mapping with an environment variable
    constant_value=None, ## For mapping with a constant value
    is_null=True ## For mapping with None

    is_required=True,
    default_value="empty",
)
Parameters
step_input_name (str): name of the input at the step level to which the deployment input will be linked to
Different possible sources:

endpoint_input_name (str, optional): name of the input at the endpoint level to which the step input will be linked to
environment_variable_name (str, optional): name of the environment variable to which the step input will be linked to
constant_value (Any, optional): a constant value to which the step input will be linked to
Other parameters:

is_null (True, optional): if specified, the input will not take any value at execution time
default_value (Any, optional): this parameter can only be specified if the deployment is an endpoint. In this case, if nothing is passed at the endpoint level, the step input will take the default_value
is_required (bool, optional): this parameter can only be specified if the deployment is an endpoint.If set to True, the corresponding endpoint input should be provided at execution time.
Return
An InputSource object that can be used in the deployment creation in the dictionary format.

Additional parameters for source definition
By default, the source is configured as an endpoint parameter, but you can configure a different source for your mapping. For each source, you have to add 1 parameter to :

Define the type of source with the name of parameter added
Precise element about the source
We will list all parameters you can have in your input mapping.

Warning

You can only add 1 parameter of source definition. By default, it's always endpoint source that is configured.

Endpoint source

Parameter name : endpoint_input_name

Source from : Outside through the endpoint

Value to put in parameter : name of the input received by the endpoint in the body of the HTTP call

Example :

endpoint_input = InputSource(
    step_input_name="seagull",
    endpoint_input_name="seagull",
    required=False, #optional 
    default="Eureka", #optional
)
Data store source

Parameter name : datastore_path

Source from : file content from the data store

Value to put in parameter : path to a data store file

Example :

data_store_input = InputSource(
    step_input_name="trainingData",
    datastore_path="path/to/trainingData.csv",
)
Example step code to read file :

def stepFileIO (trainingData) :
   print (trainingData)

   with open(trainingData["path"]) as f:
      contents = f.readlines()
      print (contents)
Constant source

Parameter name : constant_value

Source from : static value

Value to put in parameter : direct value

Example :

constant_input = InputSource(
  step_input_name="salt",
    constant_value=3,
)
Environment variable

Parameter name : environment_variable_name

Source from : the variables set at the level of an Environment in the platform

Value to put in parameter : name of the environment variable

Example :

env_var_input = InputSource(
  step_input_name="fish",
    environment_variable_name="nameOfEnvVar",
)
None value

Parameter name : no_value

Destination to : void

Value to put in parameter : True

Example :

null_input = InputSource(
  step_input_name="fish",
    is_null=True
)
Create output mapping
Import dependency
Before creating mapping between input / output of pipeline and sources / destination of endpoint, you have to import InputSource and OutputDestination objects from SDK.

from craft_ai_sdk.io import InputSource, OutputDestination
Function definition
For each output of your pipeline, in manual mapping, you have to create an object, that will be given to create_deployment() function.

endpoint_output = OutputDestination(
    step_output_name="pred_0",

    ## Choose from one of these parameters
    endpoint_output_name="pred_0", ## For mapping with an endpoint
    is_null=True ## For mapping with None
)
Parameters
step_output_name (str) - the specific output in the step
endpoint_output_name (str, optional) -- Name of the endpoint output to which the output is mapped.
is_null (True, optional) -- If specified, the output is not exposed as a deployment output.
Return
An OutputSource object who can be used in the deployment creation.

Additional parameter for destination definition
By default, the destination is configured as an endpoint parameter, but can configure a different source for your mapping. For each source, you have to add 1 parameter to :

Define the type of destination with the name of parameter added
Precise element about the destination
We will list all parameters you can have in your output mapping.

Warning

You have to add just 1 parameter of destination definition. If a parameter destination is missing, the function will generate an error (as opposed to input mapping).

Endpoint destination

Parameter name : endpoint_output_name

Destination to : Outside through the endpoint

Value to put in parameter : name of the output received by the endpoint in the body of the HTTP call

Example :

prediction_deployment_ouput = OutputDestination(
    step_output_name="prediction",
    endpoint_output_name="beautiful_prediction",
)
Data store destination

Parameter name : datastore_path

Destination to : write a file into the data store

Value to put in parameter : path to a data store folder

Example :

prediction_deployment_ouput = OutputDestination(
    step_output_name="history_prediction",
    datastore_path="path/to/history/folder.csv",
)
Example step code to send file :

def stepFileIO () :
    text_file = open('history_prediction.txt', 'wb')  # Open the file in binary mode
    text_file.write("Result of step send in file output :) ".encode('utf-8'))  # Encode the string to bytes
    text_file.close()
    fileOjb = {"history_prediction" : {"path": "history_prediction.txt"}}

   return fileOjb 
Dynamic path :

You can also specify a dynamic path for the file to be uploaded by using one of the following patterns in your datastore path:

{execution_id}: The execution id of the deployment.
{date}: The date of the execution in truncated ISO 8601 (YYYYMMDD) format.
{date_time}: The date of the execution in ISO 8601 (YYYYM-MDD_hhmmss) format.
Example with a dynamic path :

prediction_deployment_ouput = OutputDestination(
    step_output_name="history_prediction",
    datastore_path="path/to/history/exec_{execution_id}.csv",
)
Void destination

Parameter name : no_destination

Destination to : void

Value to put in parameter : True

Example :

prediction_deployment_ouput = OutputDestination(
    step_output_name="history_prediction",
    is_null=True,
)
4. Generate new endpoint token
If you need to alter the endpoint token for an endpoint, you can generate a new one with the following SDK function.

sdk.generate_new_endpoint_token(endpoint_name="*your-endpoint-name*")
Warning

This will permanently deactivate the previous token.


Choose an execution rule
A deployment is a way to run a Machine Learning pipeline in a repeatable and automated way.

For each deployment, you can configure an execution rule:

by endpoint (web API) : the pipeline will be executed by a call to a web API. In addition, this API will allow, if necessary, to retrieve data as input and deliver the result of the pipeline as output. Access to the API can be securely communicated to external users.
by periodic trigger (CRON) : rules can be configured to trigger the pipeline periodically.
Summary
Deploy with execution rule: Endpoint
Deploy with execution rule: Periodic
Function name	Method	Return type	Description
create_deployment	create_deployment(deployment_name, pipeline_name, execution_rule, mode, outputs_mapping=[], inputs_mapping=[], descriptionenable_parallel_executions=None, max_parallel_executions_per_pod=None)	Dict	Function that deploys a pipeline by creating a deployment which allows a user to trigger the pipeline execution
Deploy with execution rule: Endpoint
Definition function
To create an auto-mapping deployment where all inputs and outputs are based on API calls, you can use the create_deployment function. To create a deployment with manual mapping, you can use the create_deployment function with the additional parameters inputs_mapping to specify the precise mapping between input and source.

CraftAiSdk.create_deployment(
    pipeline_name, 
    deployment_name, 
    execution_rule="endpoint",
    mode=DEPLOYMENT_MODES.ELASTIC,
    inputs_mapping=None,
    outputs_mapping=None, 
    description=None,
    enable_parallel_executions=None,
    max_parallel_executions_per_pod=None
    )
Parameters
deployment_name (str) -- Name of endpoint chosen by the user to refer to the endpoint
pipeline_name (str) -- Name of pipeline that will be run by the deployment / endpoint
execution_rule (str) - Execution rule of the deployment. Must be endpoint or periodic. For convenience, members of the enumeration DEPLOYMENT_EXECUTION_RULES could be used too.
mode (str) â€“ Mode of the deployment. Can be "elastic" or "low_latency". Defaults to "elastic". For convenience, members of the enumeration DEPLOYMENT_MODES can be used. This defines how computing resources are allocated for pipeline executions:

elastic: Each pipeline execution runs in a new isolated container (â€œpodâ€), with its own memory (RAM, VRAM, disk). No variables or files are shared between executions, and the pod is destroyed when the execution ends. This mode is simple to use because it automatically uses computing resources for running executions, and each execution starts from an identical blank state. However, it takes time to create a new pod at the beginning of each execution (tens of seconds), and computing resources can become saturated when there are many executions.

low_latency: All pipeline executions for the same deployment run in a shared container (â€œpodâ€) with shared memory. The pod is created when the deployment is created, and deleted when the deployment is deleted. Shared memory means that if one execution modifies a global variable or a file, subsequent executions on the same pod will see the modified value. This mode allows executions to respond quickly (less than 0.5 seconds of overhead) because the pod is already up and running when an execution starts, and it is possible to preload or cache data. However, it requires care in the code because of possible interactions between executions. Additionally, computing resources must be managed carefully, as pods use resources continuously even when there is no ongoing execution, and the number of pods does not automatically adapt to the number of executions. During the lifetime of a deployment, a pod may be re-created by the platform for technical reasons (including if it tries to use more memory than available). This mode is not compatible with steps created with a container_config.dockerfile_path property in create_step().

description (str, optional) -- Text description of usage of pipeline for user only

outputs_mapping (List) - List of all OutputDestination objects with information for each output mapping.
inputs_mapping (List, optional) - List of input mappings, to map pipeline inputs to different sources (such as constant values, endpoint inputs, data store or environment variables). See InputSource for more details. For endpoint rules, if an input of the step in the pipeline is not explicitly mapped, it will be automatically mapped to an endpoint input with the same name.
description (str, optional) â€“ Description of the deployment.
enable_parallel_executions (bool, optional) â€“ Whether to run several executions at the same time in the same pod, if mode is "low_latency". Not applicable if mode is "elastic", where each execution always runs in a new pod. This is disabled by default, which means that for a deployment with "low_latency" mode, by default only one execution runs at a time on a pod, and other executions are pending while waiting for the running one to finish. Enabling this may be useful for inference batching on a model that takes much memory, so the model is loaded in memory only once and can be used for several inferences at the same time. If this is enabled, then global variables, GPU memory, and disk files are shared between multiple executions, so you must be mindful of potential race conditions and concurrency issues. For each execution running on a pod, the main Python function is run either as an asyncio coroutine with await if the function was defined with async def (recommended), or in a new thread if the function was defined simply with def. Environment variables are updated whenever a new execution starts on the pod. Using some libraries with async/threaded methods in your code may cause logs to be associated with the wrong running execution (logs are associated with executions through Python contextvars).
max_parallel_executions_per_pod (int, optional) â€“ Only applies if enable_parallel_executions is True. The maximum number of executions that can run at the same time on a deploymentâ€™s pod in "low_latency" mode where enable_parallel_executions is True: if a greater number of executions are requested at the same time, then only max_parallel_executions_per_pod executions will actually be running on the pod, and the other ones will be pending until a running execution finishes. The default is 6.
timeout_s (int) â€“ Maximum time (in seconds) to wait for the deployment to be ready. 3 minutes (180 seconds) by default, and at least 2 minutes (120 seconds).
Returns
Information about the deployment just create in a dict Python format. In this data, you will have :

name - Name of the deployment.
endpoint_token - Token of the endpoint used to trigger the deployment. Note that this token is only returned if execution_rule is â€œendpointâ€.
Example
Example auto mapping
sdk.create_deployment(
   deployment_name="my_deployment",
   pipeline_name="my_pipeline",
    execution_rule="endpoint",
   outputs_mapping=[],
   inputs_mapping=[],
)

> {
>   'name': 'name-endpoint', 
>   'endpoint_token': 'S_xZOKU ... KHs'
> }
Example manual mapping
sdk.create_deployment(
   deployment_name="my_deployment",
   pipeline_name="my_pipeline",
    execution_rule="endpoint",
    inputs_mapping=[
                seagull_endpoint_input,
                big_whale_input,
                salt_constant_input,
        ],
   outputs_mapping=[prediction_endpoint_ouput],
)


> {
>   'name': 'name-endpoint', 
>   'endpoint_token': 'S_xZOkCI ... FIg'
> }
Deploy with execution rule: Periodic
Definition function
To create an auto-mapping deployment where all inputs and outputs are based on periodicity, you can use the create_deployment function. To create a deployment with manual mapping, you can use the create_deployment function with the additional parameters inputs_mapping to specify the precise mapping between input and source.

CraftAiSdk.create_deployment(
    pipeline_name, 
    deployment_name, 
    execution_rule="periodic",
    mode=DEPLOYMENT_MODES.ELASTIC,
    schedule=None, 
    inputs_mapping=None,
    outputs_mapping=None, 
    description=None
    )
Warning

Input and output mapping must always be precise. Auto mapping isn't available for periodic deployment.

Parameters
deployment_name (str) -- Name of the deployment chosen

pipeline_name (str) -- Name of pipeline that will be run by the deployment

description (str, optional) -- Text description of usage of pipeline for user only.

execution_rule (str) - Execution rule of the deployment. Must be endpoint or periodic. For convenience, members of the enumeration DEPLOYMENT_EXECUTION_RULES could be used too.

mode (str) â€“ Mode of the deployment. Can be "elastic" or "low_latency". Defaults to "elastic". For convenience, members of the enumeration DEPLOYMENT_MODES can be used. This defines how computing resources are allocated for pipeline executions:

elastic: Each pipeline execution runs in a new isolated container (â€œpodâ€), with its own memory (RAM, VRAM, disk). No variables or files are shared between executions, and the pod is destroyed when the execution ends. This mode is simple to use because it automatically uses computing resources for running executions, and each execution starts from an identical blank state. However, it takes time to create a new pod at the beginning of each execution (tens of seconds), and computing resources can become saturated when there are many executions.

low_latency: All pipeline executions for the same deployment run in a shared container (â€œpodâ€) with shared memory. The pod is created when the deployment is created, and deleted when the deployment is deleted. Shared memory means that if one execution modifies a global variable or a file, subsequent executions on the same pod will see the modified value. This mode allows executions to respond quickly (less than 0.5 seconds of overhead) because the pod is already up and running when an execution starts, and it is possible to preload or cache data. However, it requires care in the code because of possible interactions between executions. Additionally, computing resources must be managed carefully, as pods use resources continuously even when there is no ongoing execution, and the number of pods does not automatically adapt to the number of executions. During the lifetime of a deployment, a pod may be re-created by the platform for technical reasons (including if it tries to use more memory than available). This mode is not compatible with steps created with a container_config.dockerfile_path property in create_step().

schedule (str, optional) - Schedule of the deployment. Only required if execution_rule is "periodic". Must be a valid: cron expression. The deployment will be executed periodically according to this schedule. The schedule must follow this format: <minute> <hour> <day of month> <month> <day of week>. Note that the schedule is in UTC time zone. "*" means all possible values. Here are some examples:

"0 0 * * *" will execute the deployment every day at midnight.
"0 0 5 * *" will execute the deployment every 5th day of the month at midnight.
inputs_mapping (List of instances of [InputSource], optional) - List of input mappings, to map pipeline inputs to different : sources (such as constant values, endpoint inputs, or environment variables). See InputSource for more details. For endpoint rules, if an input of the step in the pipeline is not explicitly mapped, it will be automatically mapped to an endpoint input with the same name. For periodic rules, all inputs of the step in the pipeline must be explicitly mapped.

outputs_mapping (List of instances of [OutputDestination], optional) - List of output mappings, to map pipeline outputs to different :
destinations. See OutputDestination for more details. For endpoint execution rules, if an output of the step in the pipeline is not explicitly mapped, it will be automatically mapped to an endpoint input with the same name. For other rules, all outputs of the step in the pipeline must be explicitly mapped.

description (str, optional) â€“ Description of the deployment.

enable_parallel_executions (bool, optional) â€“ Whether to run several executions at the same time in the same pod, if mode is "low_latency". Not applicable if mode is "elastic", where each execution always runs in a new pod. This is disabled by default, which means that for a deployment with "low_latency" mode, by default only one execution runs at a time on a pod, and other executions are pending while waiting for the running one to finish. Enabling this may be useful for inference batching on a model that takes much memory, so the model is loaded in memory only once and can be used for several inferences at the same time. If this is enabled, then global variables, GPU memory, and disk files are shared between multiple executions, so you must be mindful of potential race conditions and concurrency issues. For each execution running on a pod, the main Python function is run either as an asyncio coroutine with await if the function was defined with async def (recommended), or in a new thread if the function was defined simply with def. Environment variables are updated whenever a new execution starts on the pod. Using some libraries with async/threaded methods in your code may cause logs to be associated with the wrong running execution (logs are associated with executions through Python contextvars).
max_parallel_executions_per_pod (int, optional) â€“ Only applies if enable_parallel_executions is True. The maximum number of executions that can run at the same time on a deploymentâ€™s pod in "low_latency" mode where enable_parallel_executions is True: if a greater number of executions are requested at the same time, then only max_parallel_executions_per_pod executions will actually be running on the pod, and the other ones will be pending until a running execution finishes. The default is 6.
timeout_s (int) â€“ Maximum time (in seconds) to wait for the deployment to be ready. 3 minutes (180 seconds) by default, and at least 2 minutes (120 seconds).
Returns
Information about the deployment just create in a dict Python format.

name - Name of the deployment.
schedule - Schedule of the deployment. Note that this schedule is only returned if execution_rule is â€œperiodicâ€.
human_readable_schedule - Human readable schedule of the deployment. Note that this schedule is only returned if execution_rule is â€œperiodicâ€.
Example
Set up deployment to be triggered automatically every 14 days.

sdk.create_deployment(
   deployment_name="my_deployment",
   pipeline_name="my_pipeline",
   execution_rule="periodic",
   schedule="0 14 * * *"
)


> {
>   'name': 'produit-endpoint-periodic', 
>   'schedule': '*/2 * * * *', 
>   'human_readable_schedule': 'Every 2 minutes'
> }


Execute a pipeline
An execution of a pipeline creates an execution on the platform. Each execution is associated with a pipeline with the definition of the values of its inputs and outputs. The execution triggers the execution of the pipeline on one or more Kubernetes containers using the computational resources available on the environment. All the results and artifacts of the execution can be retrieved in the Execution Tracking tab.

There are two ways to execute a pipeline:

by creating a deployment: the execution will then depend on the selected execution rule and will be performed when the execution condition is met (call for an endpoint, periodicity for a CRON, etc...)
by running it instantly with the sdk: It is then necessary to indicate the values for each input of the pipeline.
Summary
Run a pipeline
Trigger a deployment with execution rule by endpoint with SDK Craft AI
Trigger a deployment with execution rule by endpoint with request
Get result of a past execution
Function name	Method	Return type	Description
run_pipeline	run_pipeline(pipeline_name, inputs=None, inputs_mapping=None, outputs_mapping=None)	dict	Executes the pipeline on the platform.
retrieve_endpoint_results	retrieve_endpoint_results(endpoint_name, execution_id, endpoint_token)	dict	Get result of endpoint execution.
Run a pipeline
A run is an execution of a pipeline on the platform. SDK function that runs a pipeline to create an execution.

run_pipeline(pipeline_name, inputs=None, inputs_mapping=None, outputs_mapping=None)
Parameters

pipeline_name (str) -- Name of an existing pipeline.
inputs (dict, optional) - Dictionary of inputs to pass to the pipeline with input names as dict keys and corresponding values as dict values. For files, the value should be the path to the file or a file content as an instance of io.IOBase. Defaults to None.
inputs_mapping (list of instances of [InputSource]{.title-ref}) - List of input mappings, to map pipeline inputs to different sources (such as environment variables). See [InputSource]{.title-ref} for more details.
outputs_mapping (list of instances of [OutputDestination]{.title-ref}) - List of output mappings, to map pipeline outputs to different destinations (such as datastore). See [OutputDestination]{.title-ref} for more details.
Returns

Created pipeline execution represented as dict with execution_id and outputs as keys. The output values will be in the output object represented as dict with output_names as keys and corresponding values as values.

Example of return object :

{
  "execution_id": "my-pipeline-8iud6",
  "outputs": {
      "output_number": 0.117,
      "output_text": "This is working fine",
   }
}
Trigger a deployment with execution rule by endpoint with SDK Craft AI
SDK function that triggers the deployment of our pipeline.

sdk.trigger_endpoint(endpoint_name, endpoint_token, inputs={},
wait_for_results=True)
Parameters

endpoint_name (str) -- Name of the endpoint.
endpoint_token (str) -- Token to access endpoint.
inputs (dict) - Inputs value for endpoint call.
wait_for_results (bool, optional) -- Automatically call retrieve_endpoint_results (True by default)
Returns

Created pipeline execution represented as dict.

Trigger a deployment with execution rule by endpoint with request
For trigger a deployment who is set up with an endpoint, you can also send request with your element defined in the pipeline input.

Examples in Python for variable :

import requests

r = requests.post(
    "https://your_environment_url/my_endpoint",
    json={
        "input1": "value1",
        "input2": [1,2,3]
        "input3": False
    },
    headers={"Authorization": "EndpointToken " + ENDPOINT_TOKEN }
)
Examples in Python for file (not available with auto mapping) :

import requests

r = requests.post(
    "https://your_environment_url/my_multistep_endpoint",
    files={"data": open("my_file.txt", "rb")},
    headers={"Authorization": "EndpointToken " + ENDPOINT_TOKEN }
)
Note

We have explained in this documentation how to trigger the endpoint with

Python, but you can obviously send a request from any tool (curl, postman, JavaScript, ...).

Warning

Inputs and outputs have size limits. This limit is 0.06MB for cumulative inputs and also 0.06MB for cumulative outputs. This input/output size limit is available for all trigger/deployment types (run, endpoint or CRON). This limit applies regardless of the source or destination of the input/output.

Only file inputs/outputs are not affected by this limit. We recommend that you use this method when transferring large amounts of data.

Get result of a past execution
Get the results of an endpoint execution.

CraftAiSdk.retrieve_endpoint_results(endpoint_name, execution_id, endpoint_token)
Parameters

endpoint_name (str) - Name of the endpoint.
execution_id (str) - Name of the execution returned by trigger_endpoint.
endpoint_token (str) - Token to access endpoint.
Returns

Created pipeline execution represented as dict with the following keys:

outputs (dict): Dictionary of outputs of the pipeline with output names as keys and corresponding values as values.
Get all execution of a pipeline
Get a list of executions for the given pipeline

CraftAiSdk.list_pipeline_executions(pipeline_name)
Parameters

pipeline_name (str) - Name of an existing pipeline.
Returns

A list of information on the pipeline execution represented as dict with the following keys:

execution_id (str): Name of the pipeline execution.
status (str): Status of the pipeline execution.
created_at (str): Date of creation of the pipeline execution.
created_by (str): ID of the user who created the pipeline execution. In the case of a pipeline run, this is the user who triggered the run. In the case of an execution via a deployment, this is the user who created the deployment.
end_date (str): Date of completion of the pipeline execution.
pipeline_name (str): Name of the pipeline used for the execution.
deployment_name (str): Name of the deployment used for the execution.
steps (list of obj): List of the step executions represented as dict with the following keys:
name (str): Name of the step.
status (str): Status of the step.
start_date (str): Date of start of the step execution.
end_date (str): Date of completion of the step execution.
commit_id (str): Id of the commit used to build the step.
repository_url (str): Url of the repository used to build the step.
repository_branch (str): Branch of the repository used to build the step.
requirements_path (str): Path of the requirements.txt file.
origin (str): The origin of the step, can be git_repository or local.


Follow the deployment executions
A deployment - is a way to run a Machine Learning pipeline in a repeatable and automated way. Once it is created, you can find the setup of your deployments in the pipeline store. In addition, you can find all the information about the executions of the deployments in the execution tracking.

Warning

Dates provided by the Web UI and SDK are always expressed in Coordinated Universal Time (UTC).

Summary
Get information about a deployment
Delete a deployment or an execution
Follow the execution tracking
Function name	Method	Return type	Description
list_deployments	list_deployments()	list of dict	Get the list of all deployments.
get_deployment	get_deployment(deployment_name)	dict	Get information of a deployment.
delete_deployment	delete_deployment(deployment_name)	dict	Delete a deployment identified by its name.
get_pipeline_execution	get_pipeline_execution(execution_id)	dict	Get the status of one pipeline execution identified by its name.
delete_pipeline_execution	delete_pipeline_execution(execution_id)	dict	Delete pipeline execution.
get_pipeline_execution_input	get_pipeline_execution_input(execution_id, input_name)	dict	Get information about an input of an execution.
get_pipeline_execution_output	get_pipeline_execution_output(execution_id, output_name)	dict	Get information about an output of an execution.
Get information about a deployment
List of deployments
Get the list of all deployments.

CraftAiSdk.list_deployments()
Returns

List of deployments represented as dict with the following keys:

name (str): Name of the deployment.
pipeline_name (str): Name of the pipeline associated to the deployment.
execution_rule (str): Execution rule of the deployment. Can be endpoint or periodic.
is_enabled (bool): Whether the deployment is enabled.
created_at (str): Date of creation of the deployment.
Get deployment information
Get information of a deployment.

CraftAiSdk.get_deployment(deployment_name)
Parameters

deployment_name (str) -- Name of the deployment.
Returns

Deployment information represented as dict with the following keys:

name (str): Name of the deployment.
mode (str): The deployment mode. Can be "elastic" or "low_latency".
pipeline (dict): Pipeline associated with the deployment represented as dict with the following keys:
name (str): Name of the pipeline.
inputs_mapping (list of dict): List of inputs mapping represented as dict with the following keys:
step_input_name (str): Name of the step input.
data_type (str): Data type of the step input.
description (str): Description of the step input.
constant_value (str): Constant value of the step input. Note that this key is only returned if the step input is mapped to a constant value.
environment_variable_name (str): Name of the environment variable. Note that this key is only returned if the step input is mapped to an environment variable.
endpoint_input_name (str): Name of the endpoint input. Note that this key is only returned if the step input is mapped to an endpoint input.
is_null (bool): Whether the step input is mapped to null. Note that this key is only returned if the step input is mapped to null.
datastore_path (str): Datastore path of the step input. Note that this key is only returned if the step input is mapped to the datastore.
is_required (bool): Whether the step input is required. Note that this key is only returned if the step input is required.
default_value (str): Default value of the step input. Note that this key is only returned if the step input has a default value.
outputs_mapping (list of dict): List of outputs mapping represented as dict with the following keys:
step_output_name (str): Name of the step output.
data_type (str): Data type of the step output.
description (str): Description of the step output.
endpoint_output_name (str): Name of the endpoint output. Note that this key is only returned if the step output is mapped to an endpoint output.
is_null (bool): Whether the step output is mapped to null. Note that this key is only returned if the step output is mapped to null.
datastore_path (str): Datastore path of the step output. Note that this key is only returned if the step output is mapped to the datastore.
endpoint_token (str): Token of the endpoint. Note that this key is only returned if the deployment is an endpoint.
schedule (str): Schedule of the deployment. Note that this key is only returned if the execution rule of the deployment is "periodic".
human_readable_schedule (str): Human readable schedule of the deployment. Note that this key is only returned if the execution rule of the deployment is "periodic".
created_at (str): Date of creation of the deployment.
created_by (str): ID of the user who created the deployment.
updated_at (str): Date of last update of the deployment.
updated_by (str): ID of the user who last updated the deployment.
last_execution_id (str): ID of the last execution of the deployment.
is_enabled (bool): Whether the deployment is enabled.
description (str): Description of the deployment.
execution_rule (str): Execution rule of the deployment.
status (str): The deployment status. Can be "pending", "up", "failed" or "standby".
pods (list of dict): List of pods associated with the low latency deployment. Note that this key is only returned if the deployment is in low latency mode. Each pod is represented as dict with the following keys:
pod_id (str): ID of the pod.
status (str): Status of the pod.
Return type

dict

Delete a deployment or an execution
Delete a deployment
Delete a deployment identified by its name.

CraftAiSdk.delete_deployment(deployment_name)
Parameters

deployment_name (str) - Name of the deployment.
Returns

Deleted deployment represented as dict (with keys name, execution_rule). The return data type is dict.

Warning

Be careful, deleting a deployment will delete all its executions.

Delete an execution
Delete one pipeline execution identified by its execution_id.

CraftAiSdk.delete_pipeline_execution(execution_id)
Parameters

execution_id (str) - Name of the pipeline execution.
Returns

Deleted pipeline execution represented as dict with the following keys:

execution_id (str): Name of the pipeline execution deleted.
Follow the execution tracking
Get execution list
Get the status of one pipeline execution identified by its name.

CraftAiSdk.get_pipeline_execution(execution_id)
Parameters

execution_id (str) - ID of the pipeline execution.
Returns

Information on the pipeline execution with id execution_id represented as dict.

execution_id (str): Name of the pipeline execution.
status (str): Status of the pipeline execution.
created_at (str): Date of creation of the pipeline
created_by (str): ID of the user who created the pipeline execution. In the case of a pipeline run, this is the user who triggered the run. In the case of an execution via a deployment, this is the user who created the deployment.
end_date (str): Date of completion of the pipeline execution.
pipeline_name (str): Name of the pipeline used for the execution.
deployment_name (str): Name of the deployment used for the execution.
steps (list of obj): List of the step executions represented as dict with the following keys:
name (str): Name of the step.
status (str): Status of the step.
start_date (str): Date of start of the step execution.
end_date (str): Date of completion of the step execution.
commit_id (str): Id of the commit used to build the step.
repository_url (str): Url of the repository used to build the step.
repository_branch (str): Branch of the repository used to build the step.
requirements_path (str): Path of the requirements.txt file.
origin (str): The origin of the step, can be git_repository or local.
inputs (list of dict): List of inputs represented as a dict with the following keys:
step_input_name (str): Name of the input.
`data_type (str): Data type of the input.
source (str): Source of type of the input. Can be environment_variable, datastore, constant, is_null endpoint or run.
endpoint_input_name (str): Name of the input in the endpoint execution if source is endpoint.
constant_value (str): Value of the constant if source is constant.
environment_variable_name (str): Name of the environment variable if source is environment_variable.
is_null (bool): True if source is is_null.
value: Value of the input.
outputs (list of dict): List of outputs represented as a dict with the following keys:
step_output_name (str): Name of the output.
`data_type (str): Data type of the output.
destination (str): Destination of type of the output. Can be datastore, is_null endpoint or run.
endpoint_output_name (str): Name of the output in the endpoint execution if destination is endpoint.
is_null (bool): True if destination is is_null.
value: Value of the output.
Get execution logs
Get the logs of an executed pipeline identified by its name.

CraftAiSdk.CraftAiSdk.get_pipeline_execution_logs(*pipeline_name*, *execution_id*,
from_datetime=None, to_datetime=None, limit=None)
Parameters

pipeline_name (str) -- Name of an existing pipeline.
execution_id (str) -- ID of the pipeline execution.
from_datetime (datetime.time, optional) -- Datetime from which the logs are collected.
to_datetime (datetime.time, optional) -- Datetime until which the logs are collected.
limit (int, optional) -- Maximum number of logs that are collected.
Returns

List of collected logs represented as dict (with keys message, timestamp and stream). The return type is a list.

Get Input and Output of an execution
Input
Get the input value of an executed pipeline identified by its execution_id.

CraftAiSdk.get_pipeline_execution_input(execution_id, input_name)
Parameters

execution_id (str) - ID of the pipeline execution.
input_name (str) - Name of the input.
Returns

Information on the input represented as a dict with the following keys :

step_input_name (str): Name of the input.
data_type (str): Data type of the input.
source (str): Source of type of the input. Can be environment_variable, datastore, constant, is_null endpoint or run.
endpoint_input_name (str): Name of the input in the endpoint execution if source is endpoint.
constant_value (str): Value of the constant if source is constant.
environment_variable_name (str): Name of the environment variable if source is environment_variable.
is_null (bool): True if source is is_null.
value: Value of the input.
Output
Get the output value of an executed pipeline identified by its execution_id.

CraftAiSdk.get_pipeline_execution_output(execution_id, output_name)
Parameters

execution_id (str) - ID of the pipeline execution.
output_name (str) - Name of the output.
Returns

Information on the output represented as a dict with the following keys :

step_output_name (str): Name of the output.
data_type (str): Data type of the output.
destination (str): Destination of type of the output. Can bedatastore,is_nullendpointorrun`.
endpoint_output_name (str): Name of the output in the endpoint ex- ecution if destination is endpoint.
is_null (bool): True if destination is is_null.
value: Value of the output.

Metrics
In the context of MLOps, tracking and monitoring metrics is critical for assessing the performance and progress of machine learning pipelines. The CraftAiSdk platform provides a comprehensive set of features for defining and recording metrics at each pipeline execution.

With measurement capabilities, you can efficiently track and retrieve the metrics associated with each execution in your machine learning pipelines. This enables you to valuable insights and make informed decisions about your models and deployments.

Pipeline Metrics
The record_metric_value function allows you to create or update a pipeline metric within a step code. This function allows you to store the name and corresponding value of a particular metric.

You do not need to declare anything outside of the step, you can just use record_metrics_value() in your step code. Remember, if you want to use the SDK in your step code, you don't need to specify your environment URL or token in the builder parameters.

After the execution is finished, you can find all your metric values in the web interface on the Execution page and on the Metrics tab.

Upload Metrics
Currently, pipeline metrics can only have one numeric value and one name for each execution metric. If multiple metrics are entered with identical names, only the last metric will be retained.

Warning

This function can only be used in the source code of the step running on the platform. When used outside a code step, it doesn't send metrics and displays a warning message.

CraftAiSdk.record_metric_value(name, value)
Parameters

name (str) - The name of the metric to store.
value (float) - The value of the metric to store.
Returns

True if sent, False otherwise

Example

Here is a very simple example of step code that sends only 2 different metrics.

Note

Don't forget to import the craft-ai-sdk package in the step code and to list the library in your requirement.txt to install it on the step execution context.

from craft_ai_sdk import CraftAiSdk

def metricsStep () :

    sdk = CraftAiSdk()

    # Some code 

    sdk.record_metric_value("accuracy", 0.1409)
    sdk.record_metric_value("loss", 1/3)

    print ("Metrics are sent")
Get metrics
The get_metrics function retrieves a list of pipeline metrics. You can filter the metrics based on the name, pipeline name, deployment name, or execution ID. It's important to note that only one of the parameters (name, pipeline_name, deployment_name, execution_id) can be set at a time.

CraftAiSdk.get_metrics(name=None, pipeline_name=None, deployment_name=None, execution_id=None)
Parameters

name (str, optional) - The name of the metric to retrieve.
pipeline_name (str, optional) - Filter metrics by pipeline. If not specified, all pipelines will be considered.
deployment_name (str, optional) - Filter metrics by deployment. If not specified, all deployments will be considered.
execution_id (str, optional) - Filter metrics by execution. If not specified, all executions will be considered.
Returns

The function returns a list of execution metrics as dictionaries. Each metric entry contains the following keys: name, value, created_at, execution_id, deployment_name, pipeline_name.

List Metrics
The Craft AI platform provides robust features for defining and recording list metrics during pipeline execution. This functionality allows you to store the name and corresponding list of values for a specific metric.

To create or update a list metric within a step code, you can utilize the record_list_metric_values() function. Afterwards, you can retrieve your metrics outside the step using the get_list_metrics() function. Additionally, you can access all your metric values in the web interface via the Metrics tab on the Execution page.

Similar to pipeline metrics, list metrics can only consist of a list of numbers (integer or float).

Upload list metrics
The record_list_metric_values() function enables you to add values to a metric list by specifying the name of the metric list and the corresponding values. There is no need to declare anything outside of the step; simply use record_list_metric_values() in your step code, as you would for pipeline metrics.

It's important to note that when using the record_list_metric_values() function, it can only be utilized within the source code of the step running on the platform. When uploading list metrics, you have the option to either specify a Python list directly or upload values individually, specifying the same metric name (which will automatically accumulate into a list).

Here is an example of step code that sends two different lists metrics:

Warning

This function can only be used in the source code of the step running on the platform. When used outside of a code step, it doesn't send metrics and displays a warning message.

CraftAiSdk.record_list_metric_values(name, values)
Parameters

name (str) - Name of the metric list to add values.
values (list of float or float) - Values of the metric list to add.
Returns

This function returns nothing (None).

Example

Here is a very simple example of step code that sends only 2 different lists metrics.

Note

Don't forget to import the craft-ai-sdk package in the step code and to list the library in your requirement.txt to install it on the step execution context.

from craft_ai_sdk import CraftAiSdk
import math 

def metricsStep():
    sdk = CraftAiSdk()

    # Some code 

    # Just one list upload
    sdk.record_list_metric_values("accuracy_list", [0.89, 0.92, 0.95])

    # Tow list upload, the lists will be concatenated in loss_list list metrics 
    sdk.record_list_metric_values("loss_list", [1.4, 1.2])
    sdk.record_list_metric_values("loss_list", [1.1, 1.0])

    # Upload multiple values that will be concatenated into 1 metrics list *logx* with all values
    for i in range (1, 50) : 

        sdk.record_list_metric_values("logx", math.log(i))

    print("List metrics are sent")
Warning

A pipeline metrics and a list metrics can have the same name in the same execution. A metrics list is limited to a maximum of 50,000 values per execution.

Get list metrics
To retrieve a list of metric lists, you can use the get_list_metrics() function. This function allows you to filter the metric lists based on the name, pipeline name, deployment name, or execution ID.

It's important to note that only one of the parameters (name, pipeline_name, deployment_name, execution_id) can be set at a time.

CraftAiSdk.get_list_metrics(name=None, pipeline_name=None, deployment_name=None, execution_id=None)
Parameters

name (str, optional) - Name of the metric list to retrieve.
pipeline_name (str, optional) - Filter metric lists by pipeline, defaults to all the pipelines.
deployment_name (str, optional) - Filter metric lists by deployment, defaults to all the deployments.
execution_id (str, optional) - Filter metric lists by execution, defaults to all the executions.
Returns

The function returns a list of execution metrics as dictionaries. Each metric entry contains the following keys: name, value, created_at, execution_id, deployment_name, pipeline_name.

Here is an example of how to use the get_list_metrics() function:

list_metrics = CraftAiSdk.get_list_metrics(name="accuracy_list", pipeline_name="my_pipeline")

